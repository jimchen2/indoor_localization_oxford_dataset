{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Tesla V100-SXM2-16GB\n",
      "1\n",
      "GPU 0: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from ipywidgets import interact, fixed\n",
    "from ipywidgets import widgets\n",
    "from ipywidgets import interactive, widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interactive, widgets, HBox, VBox\n",
    "\n",
    "from datetime import datetime\n",
    "import socket\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Proprocess Data to Raw Data for Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIXEL_TO_METER_SCALE = 13.913\n",
    "image = Image.open(\"data/map/hkust_4f.jpg\")\n",
    "image = image.resize((int(image.size[0] / PIXEL_TO_METER_SCALE), \n",
    "                      int(image.size[1] / PIXEL_TO_METER_SCALE))).transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "def process_csv_file(csv_path, image_size, pixel_to_meter_scale):\n",
    "    path_data = {'x':[], 'y':[], \"Bv\":[], \"Bh\":[], \"Bp\":[]}\n",
    "    \n",
    "    with open(csv_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Skip the header\n",
    "        \n",
    "        for row in reader:\n",
    "            x = float(row[3]) / pixel_to_meter_scale\n",
    "            y = - float(row[4]) / pixel_to_meter_scale + image_size[1]\n",
    "            Bv = float(row[0])\n",
    "            Bh = float(row[1])\n",
    "            Bp = float(row[2])\n",
    "            \n",
    "            path_data[\"x\"].append(x)\n",
    "            path_data[\"y\"].append(y)\n",
    "            path_data[\"Bv\"].append(Bv)\n",
    "            path_data[\"Bh\"].append(Bh)\n",
    "            path_data[\"Bp\"].append(Bp)\n",
    "    \n",
    "    return path_data\n",
    "\n",
    "\n",
    "train_raw_data = []\n",
    "data_path = os.path.join(\".\", \"data\", \"formatted\", \"HKUST_4F\", \"training data\")\n",
    "\n",
    "for root, _, files in os.walk(data_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            train_raw_data.append(process_csv_file(os.path.join(root, file), image.size, PIXEL_TO_METER_SCALE))\n",
    "\n",
    "test_raw_data = []\n",
    "test_data_path = os.path.join(\".\", \"data\", \"formatted\", \"HKUST_4F\", \"testing data\")\n",
    "\n",
    "for root, _, files in os.walk(test_data_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            test_raw_data.append(process_csv_file(os.path.join(root, file), image.size, PIXEL_TO_METER_SCALE))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to print the first 10 entries of a trajectory\n",
    "def print_first_10_entries(traj, dataset_name):\n",
    "    print(f\"\\n{dataset_name} Trajectory:\")\n",
    "    for i in range(min(10, len(traj['x']))):\n",
    "        print(f\"Entry {i+1}: x={traj['x'][i]:.2f}, y={traj['y'][i]:.2f}, Bv={traj['Bv'][i]:.2f}, Bh={traj['Bh'][i]:.2f}, Bp={traj['Bp'][i]:.2f}\")\n",
    "\n",
    "# Randomly select 3 trajectories from the training dataset\n",
    "print(\"Training Dataset:\")\n",
    "for i in range(3):\n",
    "    traj = random.choice(train_raw_data)\n",
    "    print_first_10_entries(traj, f\"Training Trajectory {i+1}\")\n",
    "\n",
    "# Randomly select 3 trajectories from the testing dataset\n",
    "print(\"\\nTesting Dataset:\")\n",
    "for i in range(3):\n",
    "    traj = random.choice(test_raw_data)\n",
    "    print_first_10_entries(traj, f\"Testing Trajectory {i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Analyze Differences in Distributions between Train and Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_stats(data_list, key):\n",
    "    values = np.concatenate([np.array(d[key]) for d in data_list])\n",
    "    return {\n",
    "        'mean': np.mean(values),\n",
    "        'std': np.std(values),\n",
    "        'min': np.min(values),\n",
    "        'max': np.max(values)\n",
    "    }\n",
    "\n",
    "keys = ['x', 'y', 'Bv', 'Bh', 'Bp']\n",
    "\n",
    "print(\"Training Dataset Statistics:\")\n",
    "for key in keys:\n",
    "    stats = calculate_stats(train_raw_data, key)\n",
    "    print(f\"{key}: mean={stats['mean']:.2f}, std={stats['std']:.2f}, min={stats['min']:.2f}, max={stats['max']:.2f}\")\n",
    "\n",
    "print(\"Testing Dataset Statistics:\")\n",
    "for key in keys:\n",
    "    stats = calculate_stats(test_raw_data, key)\n",
    "    print(f\"{key}: mean={stats['mean']:.2f}, std={stats['std']:.2f}, min={stats['min']:.2f}, max={stats['max']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Length of the Raw Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_data_info(data, name):\n",
    "    total_length = sum(len(d['x']) for d in data)\n",
    "    individual_lengths = [len(d['x']) for d in data]\n",
    "    num_trajectories = len(data)\n",
    "    \n",
    "    print(f\"{name} data:\")\n",
    "    print(f\"  Total length: {total_length}\")\n",
    "    print(f\"  Individual lengths: {individual_lengths}\")\n",
    "    print(f\"  Number of trajectories: {num_trajectories}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print_data_info(train_raw_data, \"Train Raw\")\n",
    "print_data_info(test_raw_data, \"Test Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_data(data):\n",
    "    def update_plot(index):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        \n",
    "        # Scatter plot for all points\n",
    "        plt.scatter(data[index][\"x\"], data[index][\"y\"], s=30, alpha=0.5, label='Steps')\n",
    "        \n",
    "        # Highlight start and end points\n",
    "        plt.scatter(data[index][\"x\"][0], data[index][\"y\"][0], color='green', s=100, label='Start')\n",
    "        plt.scatter(data[index][\"x\"][-1], data[index][\"y\"][-1], color='red', s=100, label='End')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.title(f\"Trajectory at index {index}\")\n",
    "        plt.xlabel(\"X coordinate\")\n",
    "        plt.ylabel(\"Y coordinate\")\n",
    "        plt.grid(True)\n",
    "        plt.axis('equal')  # This ensures the aspect ratio is 1:1\n",
    "        plt.show()\n",
    "\n",
    "    slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=len(data) - 1,\n",
    "        step=1,\n",
    "        description='Index:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "\n",
    "    widget = widgets.interactive(update_plot, index=slider)\n",
    "    display(widget)\n",
    "\n",
    "visualize_data(train_raw_data)\n",
    "# visualize_data(test_raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Process Data to Sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for traj in data:\n",
    "        input_seq = np.column_stack((traj['Bv'], traj['Bh'], traj['Bp']))\n",
    "        output_seq = np.column_stack((traj['x'], traj['y']))\n",
    "        \n",
    "        for i in range(len(input_seq) - sequence_length):\n",
    "            X.append(input_seq[i:i+sequence_length])\n",
    "            y.append(output_seq[i+sequence_length])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Set sequence length\n",
    "sequence_length = 30\n",
    "\n",
    "# Prepare training data\n",
    "X_train_val, y_train_val = prepare_sequences(train_raw_data, sequence_length)\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare test data\n",
    "X_test, y_test = prepare_sequences(test_raw_data, sequence_length)\n",
    "\n",
    "# Normalize input data\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_val_scaled = scaler_X.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "X_test_scaled = scaler_X.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "# Normalize output data\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_val_scaled = scaler_y.transform(y_val)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "print(\"Training data shape:\", X_train_scaled.shape, y_train_scaled.shape)\n",
    "print(\"Validation data shape:\", X_val_scaled.shape, y_val_scaled.shape)\n",
    "print(\"Testing data shape:\", X_test_scaled.shape, y_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *What you get at this step is sequence of data of `Bv`, `Bp` and `Bh` followed by an `x` and `y` output*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plot and Visualize Sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_single_dataset(index, data_type, dataset):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    if data_type == 'Raw':\n",
    "        X = X_train if dataset == 'Train' else X_val if dataset == 'Validation' else X_test\n",
    "        y = y_train if dataset == 'Train' else y_val if dataset == 'Validation' else y_test\n",
    "    else:\n",
    "        X = X_train_scaled if dataset == 'Train' else X_val_scaled if dataset == 'Validation' else X_test_scaled\n",
    "        y = y_train_scaled if dataset == 'Train' else y_val_scaled if dataset == 'Validation' else y_test_scaled\n",
    "\n",
    "    # Plot input sequence\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f'{data_type} Input Sequence - {dataset}')\n",
    "    plt.plot(X[index, :, 0], label='Bv')\n",
    "    plt.plot(X[index, :, 1], label='Bh')\n",
    "    plt.plot(X[index, :, 2], label='Bp')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Value')\n",
    "\n",
    "    # Plot output\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f'{data_type} Output - {dataset}')\n",
    "    plt.scatter(y[index, 0], y[index, 1], color='red', label='Position')\n",
    "    plt.legend()\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_interactive_plot(dataset):\n",
    "    max_index = len(X_train)-1 if dataset == 'Train' else len(X_val)-1 if dataset == 'Validation' else len(X_test)-1\n",
    "    \n",
    "    index_input = widgets.BoundedIntText(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=max_index,\n",
    "        description=f'{dataset} Index:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    index_slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=max_index,\n",
    "        description='Progress:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Link the input and slider\n",
    "    widgets.jslink((index_input, 'value'), (index_slider, 'value'))\n",
    "    \n",
    "    data_type_widget = widgets.RadioButtons(options=['Raw', 'Scaled'], description='Data Type:')\n",
    "    \n",
    "    def update_plot(index, data_type):\n",
    "        plot_single_dataset(index, data_type, dataset)\n",
    "    \n",
    "    interactive_plot = interactive(update_plot, index=index_input, data_type=data_type_widget)\n",
    "    \n",
    "    return VBox([\n",
    "        widgets.HTML(f\"<h3>{dataset} Dataset</h3>\"),\n",
    "        HBox([index_input, index_slider]),\n",
    "        data_type_widget,\n",
    "        interactive_plot.children[-1]\n",
    "    ])\n",
    "\n",
    "# Create and display interactive plots for each dataset\n",
    "train_plot = create_interactive_plot('Train')\n",
    "val_plot = create_interactive_plot('Validation')\n",
    "test_plot = create_interactive_plot('Test')\n",
    "\n",
    "display(train_plot, val_plot, test_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define the dataset and dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TrajectoryDataset(X_train_scaled, y_train_scaled)\n",
    "val_dataset = TrajectoryDataset(X_val_scaled, y_val_scaled)\n",
    "test_dataset = TrajectoryDataset(X_test_scaled, y_test_scaled)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 4096\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # It's already shuffled but doesnt hurt though\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define Training Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_original_losses(loader, model, scaler, dataset_name):\n",
    "    total_mse = 0\n",
    "    total_mae = 0\n",
    "    total_samples = 0\n",
    "    mse_criterion = nn.MSELoss(reduction='mean')\n",
    "    mae_criterion = nn.L1Loss(reduction='mean')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            \n",
    "            outputs_original = torch.from_numpy(scaler_y.inverse_transform(outputs.cpu().numpy())).to(device)\n",
    "            batch_y_original = torch.from_numpy(scaler_y.inverse_transform(batch_y.cpu().numpy())).to(device)\n",
    "            \n",
    "            mse = mse_criterion(outputs_original, batch_y_original)\n",
    "            mae = mae_criterion(outputs_original, batch_y_original)\n",
    "            \n",
    "            total_mse += mse.item() * batch_y.size(0)\n",
    "            total_mae += mae.item() * batch_y.size(0)\n",
    "            total_samples += batch_y.size(0)\n",
    "    \n",
    "    mse = total_mse / total_samples\n",
    "    mae = total_mae / total_samples\n",
    "    print(f\"Original {dataset_name} MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "    return mse, mae\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, device, num_epochs, log_original_loss_every, scaler_y, patience):\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in pbar:\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = nn.MSELoss()(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                loss =  nn.MSELoss()(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'Epoch': epoch+1,\n",
    "            'Train Loss': f'{train_loss:.4f}',\n",
    "            'Val Loss': f'{val_loss:.4f}'\n",
    "        })\n",
    "        \n",
    "        if (epoch + 1) % log_original_loss_every == 0:\n",
    "            train_mse, train_mae = calculate_original_losses(train_loader, model, scaler_y, \"Train\")\n",
    "            val_mse, val_mae = calculate_original_losses(val_loader, model, scaler_y, \"Val\")\n",
    "            # test_mse, test_mae = calculate_original_losses(test_loader, model, scaler_y, \"Test\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve == patience:\n",
    "                print(\"\\nEarly stopping!\")\n",
    "                break\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Some Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "patience = 6\n",
    "no_improve = 0\n",
    "log_original_loss_every = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Use Simple RNN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectorySimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=3):\n",
    "        super(TrajectorySimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        output = self.fc(hidden[-1])  \n",
    "        return output\n",
    "\n",
    "# Initialize the SimpleRNN model\n",
    "input_size = 3  # Bv, Bh, Bp\n",
    "hidden_size = 256\n",
    "output_size = 2  # x, y\n",
    "num_layers = 3\n",
    "simple_rnn_model = TrajectorySimpleRNN(input_size, hidden_size, output_size, num_layers=num_layers).to(device)\n",
    "\n",
    "# Define loss function and optimizer for SimpleRNN\n",
    "simple_rnn_optimizer = torch.optim.Adam(simple_rnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the SimpleRNN model\n",
    "simple_rnn_model = train_model(\n",
    "    model=simple_rnn_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=simple_rnn_optimizer,\n",
    "    device=device,\n",
    "    num_epochs=max_num_epochs,\n",
    "    log_original_loss_every=log_original_loss_every,\n",
    "    scaler_y=scaler_y,\n",
    "    patience=patience\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Use GRU RNN model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=3):  # Changed default to 3 layers\n",
    "        super(TrajectoryRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        output = self.fc(hidden[-1])  # Use the last layer's hidden state\n",
    "        return output\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 3  # Bv, Bh, Bp\n",
    "hidden_size = 256\n",
    "output_size = 2  # x, y\n",
    "num_layers = 3  # Specify the number of layers you want\n",
    "model = TrajectoryRNN(input_size, hidden_size, output_size, num_layers=num_layers).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Call the function\n",
    "model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=max_num_epochs,\n",
    "    log_original_loss_every=log_original_loss_every,\n",
    "    scaler_y=scaler_y,\n",
    "    patience=patience\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Use LSTM based Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=3):\n",
    "        super(TrajectoryLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        output = self.fc(hidden[-1])  # Use the last layer's hidden state\n",
    "        return output\n",
    "\n",
    "# Initialize the LSTM model\n",
    "input_size = 3  # Bv, Bh, Bp\n",
    "hidden_size = 256\n",
    "output_size = 2  # x, y\n",
    "num_layers = 3\n",
    "lstm_model = TrajectoryLSTM(input_size, hidden_size, output_size, num_layers=num_layers).to(device)\n",
    "\n",
    "# Define loss function and optimizer for LSTM\n",
    "lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the LSTM model\n",
    "lstm_model = train_model(\n",
    "    model=lstm_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=lstm_optimizer,\n",
    "    device=device,\n",
    "    num_epochs=max_num_epochs,\n",
    "    log_original_loss_every=log_original_loss_every,\n",
    "    scaler_y=scaler_y,\n",
    "    patience=patience\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transformer Based Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluate the model on Test Set (buggy and problematic here)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_mse, test_mae = calculate_original_losses(test_loader, model, scaler_y, \"Test\")\n",
    "\n",
    "print(f\"Final Test MSE: {test_mse:.4f}\")\n",
    "print(f\"Final Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Interactive Visulization (todo)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save the Model (todo)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# unique_id = f\"{socket.gethostname()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "# dest_path = f\"saved_models/best_model_{unique_id}.pth\"\n",
    "\n",
    "# os.makedirs('saved_models', exist_ok=True)\n",
    "# !cp best_model.pth {dest_path}\n",
    "\n",
    "# print(f\"Best model copied and saved as '{dest_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
