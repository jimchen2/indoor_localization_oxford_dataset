{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "import socket\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Proprocess Data to Raw Data for Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIXEL_TO_METER_SCALE = 13.913\n",
    "image = Image.open(\"data/map/hkust_4f.jpg\")\n",
    "image = image.resize((int(image.size[0] / PIXEL_TO_METER_SCALE), \n",
    "                      int(image.size[1] / PIXEL_TO_METER_SCALE))).transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "def process_csv_file(csv_path, image_size, pixel_to_meter_scale):\n",
    "    path_data = {'x':[], 'y':[], \"Bv\":[], \"Bh\":[], \"Bp\":[]}\n",
    "    \n",
    "    with open(csv_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Skip the header\n",
    "        \n",
    "        for row in reader:\n",
    "            x = float(row[3]) / pixel_to_meter_scale\n",
    "            y = - float(row[4]) / pixel_to_meter_scale + image_size[1]\n",
    "            Bv = float(row[0])\n",
    "            Bh = float(row[1])\n",
    "            Bp = float(row[2])\n",
    "            \n",
    "            path_data[\"x\"].append(x)\n",
    "            path_data[\"y\"].append(y)\n",
    "            path_data[\"Bv\"].append(Bv)\n",
    "            path_data[\"Bh\"].append(Bh)\n",
    "            path_data[\"Bp\"].append(Bp)\n",
    "    \n",
    "    return path_data\n",
    "\n",
    "\n",
    "train_raw_data = []\n",
    "data_path = os.path.join(\".\", \"data\", \"formatted\", \"HKUST_4F\", \"training data\")\n",
    "\n",
    "for root, _, files in os.walk(data_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            train_raw_data.append(process_csv_file(os.path.join(root, file), image.size, PIXEL_TO_METER_SCALE))\n",
    "\n",
    "test_raw_data = []\n",
    "test_data_path = os.path.join(\".\", \"data\", \"formatted\", \"HKUST_4F\", \"testing data\")\n",
    "\n",
    "for root, _, files in os.walk(test_data_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            test_raw_data.append(process_csv_file(os.path.join(root, file), image.size, PIXEL_TO_METER_SCALE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Print and Visualize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_info(data, name):\n",
    "    total_length = sum(len(d['x']) for d in data)\n",
    "    individual_lengths = [len(d['x']) for d in data]\n",
    "    num_trajectories = len(data)\n",
    "    \n",
    "    print(f\"{name} data:\")\n",
    "    print(f\"  Total length: {total_length}\")\n",
    "    print(f\"  Individual lengths: {individual_lengths}\")\n",
    "    print(f\"  Number of trajectories: {num_trajectories}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print_data_info(train_raw_data, \"Train Raw\")\n",
    "print_data_info(test_raw_data, \"Test Raw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Process Data to Sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for traj in data:\n",
    "        input_seq = np.column_stack((traj['Bv'], traj['Bh'], traj['Bp']))\n",
    "        output_seq = np.column_stack((traj['x'], traj['y']))\n",
    "        \n",
    "        for i in range(len(input_seq) - sequence_length):\n",
    "            X.append(input_seq[i:i+sequence_length])\n",
    "            y.append(output_seq[i+sequence_length])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Set sequence length\n",
    "sequence_length = 10\n",
    "\n",
    "# Prepare training data\n",
    "X_train_val, y_train_val = prepare_sequences(train_raw_data, sequence_length)\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare test data\n",
    "X_test, y_test = prepare_sequences(test_raw_data, sequence_length)\n",
    "\n",
    "# Normalize input data\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_val_scaled = scaler_X.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "X_test_scaled = scaler_X.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "# Normalize output data\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_val_scaled = scaler_y.transform(y_val)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "print(\"Training data shape:\", X_train_scaled.shape, y_train_scaled.shape)\n",
    "print(\"Validation data shape:\", X_val_scaled.shape, y_val_scaled.shape)\n",
    "print(\"Testing data shape:\", X_test_scaled.shape, y_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define the dataset and dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TrajectoryDataset(X_train_scaled, y_train_scaled)\n",
    "val_dataset = TrajectoryDataset(X_val_scaled, y_val_scaled)\n",
    "test_dataset = TrajectoryDataset(X_test_scaled, y_test_scaled)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define the RNN model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(TrajectoryRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 3  # Bv, Bh, Bp\n",
    "hidden_size = 64\n",
    "output_size = 2  # x, y\n",
    "model = TrajectoryRNN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve == patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluate the model on Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, scaler_y, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            true_values.append(batch_y.cpu().numpy())\n",
    "\n",
    "    predictions = np.concatenate(predictions)\n",
    "    true_values = np.concatenate(true_values)\n",
    "\n",
    "    predictions_original = scaler_y.inverse_transform(predictions)\n",
    "    true_values_original = scaler_y.inverse_transform(true_values)\n",
    "\n",
    "    mse = mean_squared_error(true_values_original, predictions_original)\n",
    "    mae = mean_absolute_error(true_values_original, predictions_original)\n",
    "    rmse = np.sqrt(mse)\n",
    "    max_error = np.max(np.abs(predictions_original - true_values_original))\n",
    "    r2 = r2_score(true_values_original, predictions_original)\n",
    "\n",
    "    return {\n",
    "        'predictions': predictions_original,\n",
    "        'true_values': true_values_original,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'max_error': max_error,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "# Evaluate the model that was just trained\n",
    "results = evaluate_model(model, test_loader, scaler_y, device)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMetrics for the trained model:\")\n",
    "print(f\"Mean Squared Error: {results['mse']:.4f}\")\n",
    "print(f\"Mean Absolute Error: {results['mae']:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {results['rmse']:.4f}\")\n",
    "print(f\"Maximum Absolute Error: {results['max_error']:.4f}\")\n",
    "print(f\"R-squared Score: {results['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Interactive Visulization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory(trajectory_index, results):\n",
    "    predictions = results['predictions']\n",
    "    true_values = results['true_values']\n",
    "    \n",
    "    idx = trajectory_index * 100\n",
    "    true_traj = true_values[idx:idx+100]\n",
    "    pred_traj = predictions[idx:idx+100]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(true_traj[:, 0], true_traj[:, 1], label='True', color='blue')\n",
    "    plt.plot(pred_traj[:, 0], pred_traj[:, 1], label='Predicted', color='red', linestyle='--')\n",
    "    \n",
    "    plt.title(f'Trajectory {trajectory_index}')\n",
    "    plt.legend()\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.plot(true_traj[0, 0], true_traj[0, 1], 'go', markersize=8, label='Start')\n",
    "    plt.plot(true_traj[-1, 0], true_traj[-1, 1], 'ro', markersize=8, label='End')\n",
    "    \n",
    "    mid = len(true_traj) // 2\n",
    "    plt.annotate('', xy=(true_traj[mid+1, 0], true_traj[mid+1, 1]),\n",
    "                 xytext=(true_traj[mid, 0], true_traj[mid, 1]),\n",
    "                 arrowprops=dict(facecolor='blue', shrink=0.05))\n",
    "    plt.annotate('', xy=(pred_traj[mid+1, 0], pred_traj[mid+1, 1]),\n",
    "                 xytext=(pred_traj[mid, 0], pred_traj[mid, 1]),\n",
    "                 arrowprops=dict(facecolor='red', shrink=0.05))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Create interactive plot\n",
    "max_trajectories = len(results['predictions']) // 100 - 1\n",
    "\n",
    "interact(plot_trajectory,\n",
    "         trajectory_index=widgets.IntSlider(min=0, max=max_trajectories, step=1, description='Trajectory:'),\n",
    "         results=fixed(results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id = f\"{socket.gethostname()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "model_path = os.path.join('saved_models', f\"model_{unique_id}.pth\")\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved as '{model_path}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
