{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Tesla V100-SXM2-16GB\n",
      "1\n",
      "GPU 0: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from ipywidgets import interact, fixed\n",
    "from ipywidgets import widgets\n",
    "from ipywidgets import interactive, widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interactive, widgets, HBox, VBox\n",
    "\n",
    "from datetime import datetime\n",
    "import socket\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Proprocess Data to Raw Data for Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIXEL_TO_METER_SCALE = 13.913\n",
    "image = Image.open(\"data/map/hkust_4f.jpg\")\n",
    "image = image.resize((int(image.size[0] / PIXEL_TO_METER_SCALE), \n",
    "                      int(image.size[1] / PIXEL_TO_METER_SCALE))).transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "def process_csv_file(csv_path, image_size, pixel_to_meter_scale):\n",
    "    path_data = {'x':[], 'y':[], \"Bv\":[], \"Bh\":[], \"Bp\":[]}\n",
    "    \n",
    "    with open(csv_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Skip the header\n",
    "        \n",
    "        for row in reader:\n",
    "            x = float(row[3]) / pixel_to_meter_scale\n",
    "            y = - float(row[4]) / pixel_to_meter_scale + image_size[1]\n",
    "            Bv = float(row[0])\n",
    "            Bh = float(row[1])\n",
    "            Bp = float(row[2])\n",
    "            \n",
    "            path_data[\"x\"].append(x)\n",
    "            path_data[\"y\"].append(y)\n",
    "            path_data[\"Bv\"].append(Bv)\n",
    "            path_data[\"Bh\"].append(Bh)\n",
    "            path_data[\"Bp\"].append(Bp)\n",
    "    \n",
    "    return path_data\n",
    "\n",
    "\n",
    "train_raw_data = []\n",
    "data_path = os.path.join(\".\", \"data\", \"formatted\", \"HKUST_4F\", \"training data\")\n",
    "\n",
    "for root, _, files in os.walk(data_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            train_raw_data.append(process_csv_file(os.path.join(root, file), image.size, PIXEL_TO_METER_SCALE))\n",
    "\n",
    "test_raw_data = []\n",
    "test_data_path = os.path.join(\".\", \"data\", \"formatted\", \"HKUST_4F\", \"testing data\")\n",
    "\n",
    "for root, _, files in os.walk(test_data_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            test_raw_data.append(process_csv_file(os.path.join(root, file), image.size, PIXEL_TO_METER_SCALE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Length of the Raw Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Raw data:\n",
      "  Total length: 170013\n",
      "  Individual lengths: [678, 625, 646, 669, 665, 666, 643, 686, 645, 677, 685, 690, 677, 660, 4373, 4318, 4239, 4339, 4291, 4350, 4414, 4294, 4330, 4296, 4314, 4422, 4302, 4489, 5611, 5713, 5732, 6119, 5714, 5604, 5454, 6268, 5822, 5592, 5412, 5563, 5503, 6328, 680, 685, 682, 649, 685, 710, 680, 673, 665, 684, 664, 672, 686, 670, 692, 703, 713, 675, 687, 740, 763, 700, 682, 705, 736, 710, 745, 759]\n",
      "  Number of trajectories: 70\n",
      "\n",
      "Test Raw data:\n",
      "  Total length: 24589\n",
      "  Individual lengths: [750, 5958, 658, 713, 662, 4313, 6572, 4282, 681]\n",
      "  Number of trajectories: 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_data_info(data, name):\n",
    "    total_length = sum(len(d['x']) for d in data)\n",
    "    individual_lengths = [len(d['x']) for d in data]\n",
    "    num_trajectories = len(data)\n",
    "    \n",
    "    print(f\"{name} data:\")\n",
    "    print(f\"  Total length: {total_length}\")\n",
    "    print(f\"  Individual lengths: {individual_lengths}\")\n",
    "    print(f\"  Number of trajectories: {num_trajectories}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print_data_info(train_raw_data, \"Train Raw\")\n",
    "print_data_info(test_raw_data, \"Test Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04657acdf0ac41bfa69cded25a58b351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='Index:', max=69), Output()), _d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_data(data):\n",
    "    def update_plot(index):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        \n",
    "        # Scatter plot for all points\n",
    "        plt.scatter(data[index][\"x\"], data[index][\"y\"], s=30, alpha=0.5, label='Steps')\n",
    "        \n",
    "        # Highlight start and end points\n",
    "        plt.scatter(data[index][\"x\"][0], data[index][\"y\"][0], color='green', s=100, label='Start')\n",
    "        plt.scatter(data[index][\"x\"][-1], data[index][\"y\"][-1], color='red', s=100, label='End')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.title(f\"Trajectory at index {index}\")\n",
    "        plt.xlabel(\"X coordinate\")\n",
    "        plt.ylabel(\"Y coordinate\")\n",
    "        plt.grid(True)\n",
    "        plt.axis('equal')  # This ensures the aspect ratio is 1:1\n",
    "        plt.show()\n",
    "\n",
    "    slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=len(data) - 1,\n",
    "        step=1,\n",
    "        description='Index:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "\n",
    "    widget = widgets.interactive(update_plot, index=slider)\n",
    "    display(widget)\n",
    "\n",
    "visualize_data(train_raw_data)\n",
    "# visualize_data(test_raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Process Data to Sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (134330, 30, 3) (134330, 2)\n",
      "Validation data shape: (33583, 30, 3) (33583, 2)\n",
      "Testing data shape: (24319, 30, 3) (24319, 2)\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for traj in data:\n",
    "        input_seq = np.column_stack((traj['Bv'], traj['Bh'], traj['Bp']))\n",
    "        output_seq = np.column_stack((traj['x'], traj['y']))\n",
    "        \n",
    "        for i in range(len(input_seq) - sequence_length):\n",
    "            X.append(input_seq[i:i+sequence_length])\n",
    "            y.append(output_seq[i+sequence_length])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Set sequence length\n",
    "sequence_length = 30\n",
    "\n",
    "# Prepare training data\n",
    "X_train_val, y_train_val = prepare_sequences(train_raw_data, sequence_length)\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare test data\n",
    "X_test, y_test = prepare_sequences(test_raw_data, sequence_length)\n",
    "\n",
    "# Normalize input data\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_val_scaled = scaler_X.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "X_test_scaled = scaler_X.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "# Normalize output data\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_val_scaled = scaler_y.transform(y_val)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "print(\"Training data shape:\", X_train_scaled.shape, y_train_scaled.shape)\n",
    "print(\"Validation data shape:\", X_val_scaled.shape, y_val_scaled.shape)\n",
    "print(\"Testing data shape:\", X_test_scaled.shape, y_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *What you get at this step is sequence of data of `Bv`, `Bp` and `Bh` followed by an `x` and `y` output*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plot and Visualize Sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2047e928672d473a9bae87b8b0ee142c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Train Dataset</h3>'), HBox(children=(BoundedIntText(value=0, description='Train…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2871677406ef49f9bfd78da468cfadaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Validation Dataset</h3>'), HBox(children=(BoundedIntText(value=0, description='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f61bbcdbf0047fa8a57d4c4411e43d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Test Dataset</h3>'), HBox(children=(BoundedIntText(value=0, description='Test I…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_single_dataset(index, data_type, dataset):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    if data_type == 'Raw':\n",
    "        X = X_train if dataset == 'Train' else X_val if dataset == 'Validation' else X_test\n",
    "        y = y_train if dataset == 'Train' else y_val if dataset == 'Validation' else y_test\n",
    "    else:\n",
    "        X = X_train_scaled if dataset == 'Train' else X_val_scaled if dataset == 'Validation' else X_test_scaled\n",
    "        y = y_train_scaled if dataset == 'Train' else y_val_scaled if dataset == 'Validation' else y_test_scaled\n",
    "\n",
    "    # Plot input sequence\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f'{data_type} Input Sequence - {dataset}')\n",
    "    plt.plot(X[index, :, 0], label='Bv')\n",
    "    plt.plot(X[index, :, 1], label='Bh')\n",
    "    plt.plot(X[index, :, 2], label='Bp')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Value')\n",
    "\n",
    "    # Plot output\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f'{data_type} Output - {dataset}')\n",
    "    plt.scatter(y[index, 0], y[index, 1], color='red', label='Position')\n",
    "    plt.legend()\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_interactive_plot(dataset):\n",
    "    max_index = len(X_train)-1 if dataset == 'Train' else len(X_val)-1 if dataset == 'Validation' else len(X_test)-1\n",
    "    \n",
    "    index_input = widgets.BoundedIntText(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=max_index,\n",
    "        description=f'{dataset} Index:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    index_slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=max_index,\n",
    "        description='Progress:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Link the input and slider\n",
    "    widgets.jslink((index_input, 'value'), (index_slider, 'value'))\n",
    "    \n",
    "    data_type_widget = widgets.RadioButtons(options=['Raw', 'Scaled'], description='Data Type:')\n",
    "    \n",
    "    def update_plot(index, data_type):\n",
    "        plot_single_dataset(index, data_type, dataset)\n",
    "    \n",
    "    interactive_plot = interactive(update_plot, index=index_input, data_type=data_type_widget)\n",
    "    \n",
    "    return VBox([\n",
    "        widgets.HTML(f\"<h3>{dataset} Dataset</h3>\"),\n",
    "        HBox([index_input, index_slider]),\n",
    "        data_type_widget,\n",
    "        interactive_plot.children[-1]\n",
    "    ])\n",
    "\n",
    "# Create and display interactive plots for each dataset\n",
    "train_plot = create_interactive_plot('Train')\n",
    "val_plot = create_interactive_plot('Validation')\n",
    "test_plot = create_interactive_plot('Test')\n",
    "\n",
    "display(train_plot, val_plot, test_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define the dataset and dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: torch.Size([134330, 30, 3]) torch.Size([134330, 2])\n",
      "Validation dataset shape: torch.Size([33583, 30, 3]) torch.Size([33583, 2])\n",
      "Test dataset shape: torch.Size([24319, 30, 3]) torch.Size([24319, 2])\n"
     ]
    }
   ],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TrajectoryDataset(X_train_scaled, y_train_scaled)\n",
    "val_dataset = TrajectoryDataset(X_val_scaled, y_val_scaled)\n",
    "test_dataset = TrajectoryDataset(X_test_scaled, y_test_scaled)\n",
    "\n",
    "# Print dataset shapes\n",
    "print(\"Train dataset shape:\", train_dataset.X.shape, train_dataset.y.shape)\n",
    "print(\"Validation dataset shape:\", val_dataset.X.shape, val_dataset.y.shape)\n",
    "print(\"Test dataset shape:\", test_dataset.X.shape, test_dataset.y.shape)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # It's already shuffled but doesnt hurt though\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define the RNN model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(TrajectoryRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        output = self.fc(hidden[-1])  # Use the last layer's hidden state\n",
    "        return output\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 3  # Bv, Bh, Bp\n",
    "hidden_size = 256\n",
    "output_size = 2  # x, y\n",
    "model = TrajectoryRNN(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/500 [00:08<?, ?it/s, Epoch=1, Train Loss=0.9242, Val Loss=0.8827]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 915.1847, MAE: 22.5132\n",
      "Original Val MSE: 921.8801, MAE: 22.5692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 1/500 [00:15<2:06:27, 15.21s/it, Epoch=1, Train Loss=0.9242, Val Loss=0.8827]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test MSE: 917.5258, MAE: 22.6963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 1/500 [00:23<2:06:27, 15.21s/it, Epoch=2, Train Loss=0.8619, Val Loss=0.8491]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 886.3395, MAE: 21.8318\n",
      "Original Val MSE: 894.5422, MAE: 21.9047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 2/500 [00:30<2:05:38, 15.14s/it, Epoch=2, Train Loss=0.8619, Val Loss=0.8491]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test MSE: 950.9402, MAE: 23.0232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 2/500 [00:38<2:05:38, 15.14s/it, Epoch=3, Train Loss=0.8148, Val Loss=0.7988]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 839.6301, MAE: 21.0617\n",
      "Original Val MSE: 852.5300, MAE: 21.1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 3/500 [00:45<2:05:11, 15.11s/it, Epoch=3, Train Loss=0.8148, Val Loss=0.7988]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test MSE: 889.7177, MAE: 21.8876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 3/500 [00:53<2:05:11, 15.11s/it, Epoch=4, Train Loss=0.7763, Val Loss=0.7674]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 812.0565, MAE: 20.5092\n",
      "Original Val MSE: 824.7307, MAE: 20.6325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 4/500 [01:00<2:04:58, 15.12s/it, Epoch=4, Train Loss=0.7763, Val Loss=0.7674]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test MSE: 932.0046, MAE: 22.1245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 4/500 [01:08<2:04:58, 15.12s/it, Epoch=5, Train Loss=0.7449, Val Loss=0.7366]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 788.5672, MAE: 19.9148\n",
      "Original Val MSE: 802.3471, MAE: 20.0651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 5/500 [01:15<2:04:23, 15.08s/it, Epoch=5, Train Loss=0.7449, Val Loss=0.7366]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test MSE: 924.6388, MAE: 21.8934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 5/500 [01:23<2:04:23, 15.08s/it, Epoch=6, Train Loss=0.7194, Val Loss=0.7176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 762.0949, MAE: 19.3777\n",
      "Original Val MSE: 779.6486, MAE: 19.5880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 6/500 [01:30<2:03:44, 15.03s/it, Epoch=6, Train Loss=0.7194, Val Loss=0.7176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test MSE: 942.6679, MAE: 22.1340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 6/500 [01:38<2:03:44, 15.03s/it, Epoch=7, Train Loss=0.6924, Val Loss=0.6891]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 736.8219, MAE: 18.7926\n",
      "Original Val MSE: 758.0806, MAE: 19.0773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|▏         | 7/500 [01:45<2:03:17, 15.00s/it, Epoch=7, Train Loss=0.6924, Val Loss=0.6891]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test MSE: 929.1143, MAE: 21.6575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|▏         | 7/500 [01:53<2:03:17, 15.00s/it, Epoch=8, Train Loss=0.6676, Val Loss=0.6631]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 715.3904, MAE: 18.5351\n",
      "Original Val MSE: 734.3995, MAE: 18.7765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 8/500 [02:00<2:02:46, 14.97s/it, Epoch=8, Train Loss=0.6676, Val Loss=0.6631]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test MSE: 914.8088, MAE: 21.5748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 8/500 [02:08<2:02:46, 14.97s/it, Epoch=9, Train Loss=0.6410, Val Loss=0.6416]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 685.5409, MAE: 17.8083\n",
      "Original Val MSE: 712.2463, MAE: 18.1596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 9/500 [02:15<2:02:26, 14.96s/it, Epoch=9, Train Loss=0.6410, Val Loss=0.6416]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test MSE: 962.1996, MAE: 21.6231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 9/500 [02:23<2:02:26, 14.96s/it, Epoch=10, Train Loss=0.6182, Val Loss=0.6191]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 659.3738, MAE: 17.5520\n",
      "Original Val MSE: 684.4684, MAE: 17.9091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 10/500 [02:30<2:02:30, 15.00s/it, Epoch=10, Train Loss=0.6182, Val Loss=0.6191]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test MSE: 928.3611, MAE: 21.5665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 10/500 [02:38<2:02:30, 15.00s/it, Epoch=11, Train Loss=0.5928, Val Loss=0.5950]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 630.5349, MAE: 16.9139\n",
      "Original Val MSE: 662.9278, MAE: 17.3552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 11/500 [02:45<2:02:28, 15.03s/it, Epoch=11, Train Loss=0.5928, Val Loss=0.5950]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test MSE: 990.7872, MAE: 22.1090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 11/500 [02:53<2:02:28, 15.03s/it, Epoch=12, Train Loss=0.5676, Val Loss=0.5716]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 602.2874, MAE: 16.4812\n",
      "Original Val MSE: 633.1440, MAE: 16.9280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 12/500 [03:00<2:02:22, 15.05s/it, Epoch=12, Train Loss=0.5676, Val Loss=0.5716]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test MSE: 955.1209, MAE: 21.6119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 12/500 [03:09<2:02:22, 15.05s/it, Epoch=13, Train Loss=0.5436, Val Loss=0.5447]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 569.4186, MAE: 15.9269\n",
      "Original Val MSE: 605.6875, MAE: 16.4778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 13/500 [03:15<2:02:12, 15.06s/it, Epoch=13, Train Loss=0.5436, Val Loss=0.5447]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Test MSE: 957.5242, MAE: 21.6797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 13/500 [03:24<2:02:12, 15.06s/it, Epoch=14, Train Loss=0.5193, Val Loss=0.5231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 542.8138, MAE: 15.4606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 13/500 [03:29<2:11:06, 16.15s/it, Epoch=14, Train Loss=0.5193, Val Loss=0.5231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 582.6044, MAE: 16.0518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m     train_mse, train_mae \u001b[38;5;241m=\u001b[39m calculate_original_losses(train_loader, model, scaler_y, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m     val_mse, val_mae \u001b[38;5;241m=\u001b[39m calculate_original_losses(val_loader, model, scaler_y, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m     test_mse, test_mae \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_original_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n\u001b[1;32m     77\u001b[0m     best_val_loss \u001b[38;5;241m=\u001b[39m val_loss\n",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m, in \u001b[0;36mcalculate_original_losses\u001b[0;34m(loader, model, scaler, dataset_name)\u001b[0m\n\u001b[1;32m     12\u001b[0m mae_criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mL1Loss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_num_epochs = 500\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "no_improve = 0\n",
    "log_original_loss_every = 1\n",
    "\n",
    "def calculate_original_losses(loader, model, scaler, dataset_name):\n",
    "    total_mse = 0\n",
    "    total_mae = 0\n",
    "    total_samples = 0\n",
    "    mse_criterion = nn.MSELoss(reduction='mean')\n",
    "    mae_criterion = nn.L1Loss(reduction='mean')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            \n",
    "            outputs_original = torch.from_numpy(scaler_y.inverse_transform(outputs.cpu().numpy())).to(device)\n",
    "            batch_y_original = torch.from_numpy(scaler_y.inverse_transform(batch_y.cpu().numpy())).to(device)\n",
    "            \n",
    "            mse = mse_criterion(outputs_original, batch_y_original)\n",
    "            mae = mae_criterion(outputs_original, batch_y_original)\n",
    "            \n",
    "            total_mse += mse.item() * batch_y.size(0)\n",
    "            total_mae += mae.item() * batch_y.size(0)\n",
    "            total_samples += batch_y.size(0)\n",
    "    \n",
    "    mse = total_mse / total_samples\n",
    "    mae = total_mae / total_samples\n",
    "    print(f\"Original {dataset_name} MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "    return mse, mae\n",
    "\n",
    "pbar = tqdm(range(max_num_epochs), desc=\"Training Progress\")\n",
    "\n",
    "\n",
    "\n",
    "for epoch in pbar:\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = nn.MSELoss()(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss =  nn.MSELoss()(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    pbar.set_postfix({\n",
    "        'Epoch': epoch+1,\n",
    "        'Train Loss': f'{train_loss:.4f}',\n",
    "        'Val Loss': f'{val_loss:.4f}'\n",
    "    })\n",
    "    \n",
    "    if (epoch + 1) % log_original_loss_every == 0:\n",
    "        train_mse, train_mae = calculate_original_losses(train_loader, model, scaler_y, \"Train\")\n",
    "        val_mse, val_mae = calculate_original_losses(val_loader, model, scaler_y, \"Val\")\n",
    "        test_mse, test_mae = calculate_original_losses(test_loader, model, scaler_y, \"Test\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve == patience:\n",
    "            print(\"\\nEarly stopping!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluate the model on Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_mse, test_mae = calculate_original_losses(test_loader, model, scaler_y, \"Test\")\n",
    "\n",
    "print(f\"Final Test MSE: {test_mse:.4f}\")\n",
    "print(f\"Final Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Interactive Visulization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id = f\"{socket.gethostname()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "dest_path = f\"saved_models/best_model_{unique_id}.pth\"\n",
    "\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "!cp best_model.pth {dest_path}\n",
    "\n",
    "print(f\"Best model copied and saved as '{dest_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
