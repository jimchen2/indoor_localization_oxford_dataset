{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import keras\n",
    "from tcn import TCN\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime\n",
    "import socket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LD_LIBRARY_PATH set to: /usr/lib/cuda/lib64\n",
      "TensorFlow version: 2.17.0\n",
      "CUDA_HOME: /usr/lib/cuda\n",
      "LD_LIBRARY_PATH: /usr/lib/cuda/lib64\n",
      "Num GPUs Available: 1\n",
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1720929313.955252   60859 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1720929314.023086   60859 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1720929314.023413   60859 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "os.environ['CUDA_HOME'] = '/usr/lib/cuda'\n",
    "cuda_lib_path = os.path.join(os.environ['CUDA_HOME'], 'lib64')\n",
    "if 'LD_LIBRARY_PATH' in os.environ:\n",
    "    os.environ['LD_LIBRARY_PATH'] = f\"{cuda_lib_path}:{os.environ['LD_LIBRARY_PATH']}\"\n",
    "else:\n",
    "    os.environ['LD_LIBRARY_PATH'] = cuda_lib_path\n",
    "\n",
    "print(\"LD_LIBRARY_PATH set to:\", os.environ['LD_LIBRARY_PATH'])\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"CUDA_HOME:\", os.environ.get('CUDA_HOME'))\n",
    "print(\"LD_LIBRARY_PATH:\", os.environ.get('LD_LIBRARY_PATH'))\n",
    "print(\"Num GPUs Available:\", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_flipped = False\n",
    "WINDOW_SIZE = 10\n",
    "BATCH_SIZE = 32\n",
    "patience = 100\n",
    "Partition_Of_DataSets = 1\n",
    "\n",
    "GAUSSIAN_NOISE_SD = 0.1\n",
    "MAX_ROTATION_ANGLE_NOISE = 0\n",
    "STORM_INTENSITY_NOISE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIXEL_TO_METER_SCALE = 13.913\n",
    "image = Image.open(\"data/map/hkust_4f.jpg\")\n",
    "image = image.resize((int(image.size[0] / PIXEL_TO_METER_SCALE), int(image.size[1] / PIXEL_TO_METER_SCALE))).transpose(Image.FLIP_TOP_BOTTOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_file(csv_path, image_size, pixel_to_meter_scale):\n",
    "    path_data = {'x':[], 'y':[], \"Bv\":[], \"Bh\":[], \"Bp\":[]}\n",
    "    \n",
    "    with open(csv_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Skip the header\n",
    "        \n",
    "        for row in reader:\n",
    "            x = float(row[3]) / pixel_to_meter_scale\n",
    "            y = - float(row[4]) / pixel_to_meter_scale + image_size[1]\n",
    "            Bv = float(row[0])\n",
    "            Bh = float(row[1])\n",
    "            Bp = float(row[2])\n",
    "            \n",
    "            path_data[\"x\"].append(x)\n",
    "            path_data[\"y\"].append(y)\n",
    "            path_data[\"Bv\"].append(Bv)\n",
    "            path_data[\"Bh\"].append(Bh)\n",
    "            path_data[\"Bp\"].append(Bp)\n",
    "    \n",
    "    return path_data\n",
    "\n",
    "\n",
    "train_raw_data = []\n",
    "data_path = os.path.join(\".\", \"data\", \"formatted\", \"HKUST_4F\", \"training data\")\n",
    "\n",
    "for root, _, files in os.walk(data_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            train_raw_data.append(process_csv_file(os.path.join(root, file), image.size, PIXEL_TO_METER_SCALE))\n",
    "\n",
    "test_raw_data = []\n",
    "test_data_path = os.path.join(\".\", \"data\", \"formatted\", \"HKUST_4F\", \"testing data\")\n",
    "\n",
    "for root, _, files in os.walk(test_data_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            test_raw_data.append(process_csv_file(os.path.join(root, file), image.size, PIXEL_TO_METER_SCALE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Raw data:\n",
      "  Total length: 170013\n",
      "  Individual lengths: [678, 625, 646, 669, 665, 666, 643, 686, 645, 677, 685, 690, 677, 660, 4373, 4318, 4239, 4339, 4291, 4350, 4414, 4294, 4330, 4296, 4314, 4422, 4302, 4489, 5611, 5713, 5732, 6119, 5714, 5604, 5454, 6268, 5822, 5592, 5412, 5563, 5503, 6328, 680, 685, 682, 649, 685, 710, 680, 673, 665, 684, 664, 672, 686, 670, 692, 703, 713, 675, 687, 740, 763, 700, 682, 705, 736, 710, 745, 759]\n",
      "  Number of trajectories: 70\n",
      "\n",
      "Test Raw data:\n",
      "  Total length: 24589\n",
      "  Individual lengths: [750, 5958, 658, 713, 662, 4313, 6572, 4282, 681]\n",
      "  Number of trajectories: 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_data_info(data, name):\n",
    "    total_length = sum(len(d['x']) for d in data)\n",
    "    individual_lengths = [len(d['x']) for d in data]\n",
    "    num_trajectories = len(data)\n",
    "    \n",
    "    print(f\"{name} data:\")\n",
    "    print(f\"  Total length: {total_length}\")\n",
    "    print(f\"  Individual lengths: {individual_lengths}\")\n",
    "    print(f\"  Number of trajectories: {num_trajectories}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print_data_info(train_raw_data, \"Train Raw\")\n",
    "print_data_info(test_raw_data, \"Test Raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "valid_data = []\n",
    "valid_labels = []\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "\n",
    "\n",
    "def split_data(data, partition=4):\n",
    "    result = []\n",
    "    for traj in data:\n",
    "        for i in range(partition):\n",
    "            new_traj = {key: value[i::partition] for key, value in traj.items()}\n",
    "            result.append(new_traj)\n",
    "    return result\n",
    "def prepare_datasets(train_data, test_data, partition=4, val_split=0.2):\n",
    "    train_processed = split_data(train_data, partition)\n",
    "    test_processed = split_data(test_data, partition)\n",
    "    train_data, val_data = train_test_split(train_processed, test_size=val_split, random_state=42)\n",
    "    return train_data, val_data, test_processed\n",
    "\n",
    "train_data, valid_data, test_data = prepare_datasets(train_raw_data, test_raw_data, partition=Partition_Of_DataSets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "  Total length: 142903\n",
      "  Individual lengths: [763, 5732, 713, 6268, 703, 710, 4239, 5454, 680, 5611, 686, 672, 5503, 682, 685, 4350, 692, 5563, 4422, 5412, 660, 665, 669, 4339, 645, 670, 643, 5822, 682, 759, 745, 4318, 4489, 6328, 4302, 680, 4314, 675, 700, 690, 5714, 736, 740, 5592, 5713, 685, 705, 625, 664, 4294, 646, 4296, 4414, 687, 4373, 684]\n",
      "  Number of trajectories: 56\n",
      "\n",
      "Validation data:\n",
      "  Total length: 27110\n",
      "  Individual lengths: [4330, 678, 673, 665, 686, 4291, 685, 5604, 649, 677, 6119, 677, 710, 666]\n",
      "  Number of trajectories: 14\n",
      "\n",
      "Test data:\n",
      "  Total length: 24589\n",
      "  Individual lengths: [750, 5958, 658, 713, 662, 4313, 6572, 4282, 681]\n",
      "  Number of trajectories: 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_data_info(train_data, \"Train\")\n",
    "print_data_info(valid_data, \"Validation\")\n",
    "print_data_info(test_data, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_vector(v, theta):\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    return np.dot(np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]]), v)\n",
    "\n",
    "def magnetic_storm(t, amplitude, frequency):\n",
    "    return amplitude * np.sin(2 * np.pi * frequency * t) * np.exp(-t)\n",
    "\n",
    "def process_dataset(dataset, is_training=False, add_flipped=False, gaussian_noise_sd=0, max_rotation_angle=0, storm_intensity=0):\n",
    "    data, labels = [], []\n",
    "    \n",
    "    for d in dataset:\n",
    "        \n",
    "        if len(d['Bv']) < WINDOW_SIZE:\n",
    "            continue\n",
    "        \n",
    "        for window_idx in range(len(d['Bv']) - WINDOW_SIZE + 1):\n",
    "            window = slice(window_idx, window_idx + WINDOW_SIZE)\n",
    "            B_data = np.column_stack((d['Bv'][window], d['Bh'][window], d['Bp'][window]))\n",
    "\n",
    "            if is_training:\n",
    "                if gaussian_noise_sd > 0:\n",
    "                    B_data += np.random.normal(0, gaussian_noise_sd, B_data.shape)\n",
    "                \n",
    "                if max_rotation_angle > 0:\n",
    "                    theta = np.random.uniform(-max_rotation_angle, max_rotation_angle)\n",
    "                    B_data = np.apply_along_axis(lambda v: rotate_vector(v, theta), 1, B_data)\n",
    "\n",
    "                if storm_intensity > 0:\n",
    "                    t = np.linspace(0, 1, WINDOW_SIZE)\n",
    "                    storm = magnetic_storm(t, storm_intensity, np.random.uniform(0.5, 2.0))\n",
    "                    B_data += storm[:, np.newaxis]\n",
    "\n",
    "            data.append(B_data)\n",
    "            labels.append([d['x'][window.stop-1], d['y'][window.stop-1]])\n",
    "\n",
    "            if add_flipped:\n",
    "                data.append(np.flip(B_data, axis=0))\n",
    "                labels.append([d['x'][window.start], d['y'][window.start]])\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_data: (142399, 10, 3)\n",
      "shape of train_labels: (142399, 2)\n",
      "shape of valid_data: (26984, 10, 3)\n",
      "shape of valid_labels: (26984, 2)\n",
      "shape of test_data: (24508, 10, 3)\n",
      "shape of test_labels: (24508, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1720929317.555497   60859 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1720929317.555790   60859 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1720929317.555977   60859 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1720929317.749330   60859 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1720929317.749586   60859 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1720929317.749791   60859 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "# Process datasets\n",
    "train_data, train_labels = process_dataset(train_data, is_training=True)\n",
    "valid_data, valid_labels = process_dataset(valid_data, is_training=False)\n",
    "test_data, test_labels = process_dataset(\n",
    "    test_data, \n",
    "    is_training=False, \n",
    "    add_flipped=False,\n",
    "    gaussian_noise_sd=GAUSSIAN_NOISE_SD,\n",
    "    max_rotation_angle=MAX_ROTATION_ANGLE_NOISE,\n",
    "    storm_intensity=STORM_INTENSITY_NOISE,\n",
    ")\n",
    "\n",
    "print(f\"shape of train_data: {train_data.shape}\")\n",
    "print(f\"shape of train_labels: {train_labels.shape}\")\n",
    "print(f\"shape of valid_data: {valid_data.shape}\")\n",
    "print(f\"shape of valid_labels: {valid_labels.shape}\")\n",
    "print(f\"shape of test_data: {test_data.shape}\")\n",
    "print(f\"shape of test_labels: {test_labels.shape}\")\n",
    "\n",
    "# Convert data to TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_data, valid_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_FILTERS = 8\n",
    "DILATIONS = [1, 2, 4, 8, 16]\n",
    "DENSE_UNITS = [16, 8, 4, 2]\n",
    "LEAKY_RELU_ALPHA = 0.1\n",
    "LOSS = 'mae'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/myenv/lib/python3.12/site-packages/keras/src/layers/normalization/batch_normalization.py:143: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/root/myenv/lib/python3.12/site-packages/tcn/tcn.py:227: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super(TCN, self).__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "def create_model(nb_filters, dilations, dense_units, leaky_relu_alpha, loss):\n",
    "    # Default Adam optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.BatchNormalization(input_shape=(WINDOW_SIZE, 3)),\n",
    "        TCN(input_shape=(WINDOW_SIZE, 3), nb_filters=nb_filters, return_sequences=True, dilations=dilations),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.LeakyReLU(leaky_relu_alpha)\n",
    "    ])\n",
    "    for units in dense_units[:-1]:  # All but the last dense layer\n",
    "        model.add(keras.layers.Dense(units))\n",
    "        model.add(keras.layers.BatchNormalization())\n",
    "        model.add(keras.layers.LeakyReLU(leaky_relu_alpha))\n",
    "    model.add(keras.layers.Dense(dense_units[-1]))\n",
    "    \n",
    "    # Compile the model with additional metrics\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss, \n",
    "                  metrics=[\n",
    "                      keras.metrics.MeanAbsoluteError(),\n",
    "                      keras.metrics.MeanSquaredError(),\n",
    "                      keras.metrics.RootMeanSquaredError(),\n",
    "                      keras.metrics.MeanAbsolutePercentageError()\n",
    "                  ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model using the defined hyperparameters\n",
    "model = create_model(\n",
    "    nb_filters=NB_FILTERS,\n",
    "    dilations=DILATIONS,\n",
    "    dense_units=DENSE_UNITS,\n",
    "    leaky_relu_alpha=LEAKY_RELU_ALPHA,\n",
    "    loss=LOSS\n",
    ")\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "hostname = socket.gethostname()\n",
    "log_dir = f'logs/experiment_{current_time}_{hostname}'\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - loss: 96.9651 - mean_absolute_error: 96.9651 - mean_absolute_percentage_error: 57.5961 - mean_squared_error: 12219.1621 - root_mean_squared_error: 110.4912 - val_loss: 70.4214 - val_mean_absolute_error: 70.4214 - val_mean_absolute_percentage_error: 37.4476 - val_mean_squared_error: 7624.7998 - val_root_mean_squared_error: 87.3201\n",
      "Epoch 2/1000\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 83.5815 - mean_absolute_error: 83.5815 - mean_absolute_percentage_error: 49.3642 - mean_squared_error: 9867.4707 - root_mean_squared_error: 99.3137 - val_loss: 52.3876 - val_mean_absolute_error: 52.3876 - val_mean_absolute_percentage_error: 25.7379 - val_mean_squared_error: 5430.0996 - val_root_mean_squared_error: 73.6892\n",
      "Epoch 3/1000\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 72.8809 - mean_absolute_error: 72.8809 - mean_absolute_percentage_error: 41.3273 - mean_squared_error: 8598.4785 - root_mean_squared_error: 92.6771 - val_loss: 46.5539 - val_mean_absolute_error: 46.5539 - val_mean_absolute_percentage_error: 22.1704 - val_mean_squared_error: 4682.9585 - val_root_mean_squared_error: 68.4321\n",
      "Epoch 4/1000\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 61.2526 - mean_absolute_error: 61.2526 - mean_absolute_percentage_error: 34.4893 - mean_squared_error: 6691.5942 - root_mean_squared_error: 81.7411 - val_loss: 41.3082 - val_mean_absolute_error: 41.3082 - val_mean_absolute_percentage_error: 19.8342 - val_mean_squared_error: 3689.7725 - val_root_mean_squared_error: 60.7435\n",
      "Epoch 5/1000\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 55.9670 - mean_absolute_error: 55.9670 - mean_absolute_percentage_error: 31.1510 - mean_squared_error: 5707.2319 - root_mean_squared_error: 75.5319 - val_loss: 37.6722 - val_mean_absolute_error: 37.6722 - val_mean_absolute_percentage_error: 18.7296 - val_mean_squared_error: 3190.5132 - val_root_mean_squared_error: 56.4846\n",
      "Epoch 6/1000\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 49.8819 - mean_absolute_error: 49.8819 - mean_absolute_percentage_error: 27.9659 - mean_squared_error: 4668.2251 - root_mean_squared_error: 68.2958 - val_loss: 37.0885 - val_mean_absolute_error: 37.0885 - val_mean_absolute_percentage_error: 18.1814 - val_mean_squared_error: 3028.0674 - val_root_mean_squared_error: 55.0279\n",
      "Epoch 7/1000\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 45.0659 - mean_absolute_error: 45.0659 - mean_absolute_percentage_error: 25.2163 - mean_squared_error: 3862.4741 - root_mean_squared_error: 62.1258 - val_loss: 34.6367 - val_mean_absolute_error: 34.6367 - val_mean_absolute_percentage_error: 17.7103 - val_mean_squared_error: 2610.2954 - val_root_mean_squared_error: 51.0910\n",
      "Epoch 8/1000\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 41.3009 - mean_absolute_error: 41.3009 - mean_absolute_percentage_error: 23.2015 - mean_squared_error: 3347.3667 - root_mean_squared_error: 57.7646 - val_loss: 34.6833 - val_mean_absolute_error: 34.6833 - val_mean_absolute_percentage_error: 17.8389 - val_mean_squared_error: 2771.6904 - val_root_mean_squared_error: 52.6468\n",
      "Epoch 9/1000\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 37.3139 - mean_absolute_error: 37.3139 - mean_absolute_percentage_error: 21.1657 - mean_squared_error: 2661.9377 - root_mean_squared_error: 51.5434 - val_loss: 33.9161 - val_mean_absolute_error: 33.9161 - val_mean_absolute_percentage_error: 17.8946 - val_mean_squared_error: 2835.7180 - val_root_mean_squared_error: 53.2515\n",
      "Epoch 10/1000\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 34.0556 - mean_absolute_error: 34.0556 - mean_absolute_percentage_error: 19.3896 - mean_squared_error: 2245.8784 - root_mean_squared_error: 47.3814 - val_loss: 34.6931 - val_mean_absolute_error: 34.6931 - val_mean_absolute_percentage_error: 18.7973 - val_mean_squared_error: 3765.2380 - val_root_mean_squared_error: 61.3615\n",
      "Epoch 11/1000\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 32.9738 - mean_absolute_error: 32.9738 - mean_absolute_percentage_error: 18.1554 - mean_squared_error: 2071.0444 - root_mean_squared_error: 45.4587 - val_loss: 34.3682 - val_mean_absolute_error: 34.3682 - val_mean_absolute_percentage_error: 18.2001 - val_mean_squared_error: 3557.3918 - val_root_mean_squared_error: 59.6439\n",
      "Epoch 12/1000\n",
      "\u001b[1m 50/250\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 29.6986 - mean_absolute_error: 29.6986 - mean_absolute_percentage_error: 16.3832 - mean_squared_error: 1786.9066 - root_mean_squared_error: 42.1709"
     ]
    }
   ],
   "source": [
    "# Subset training parameters\n",
    "subset_train_size = 1000\n",
    "subset_val_size = 200\n",
    "subset_batch_size = 4\n",
    "subset_epochs = 100\n",
    "subset_model_path = 'saved_model/subset_model.keras'\n",
    "\n",
    "# Create subsets\n",
    "train_indices = np.random.choice(train_data.shape[0], size=subset_train_size, replace=False)\n",
    "subset_train_data = train_data[train_indices]\n",
    "subset_train_labels = train_labels[train_indices]\n",
    "\n",
    "val_indices = np.random.choice(valid_data.shape[0], size=subset_val_size, replace=False)\n",
    "subset_val_data = valid_data[val_indices]\n",
    "subset_val_labels = valid_labels[val_indices]\n",
    "\n",
    "# Train on subset\n",
    "subset_checkpoint = ModelCheckpoint(subset_model_path, monitor='val_mean_absolute_error', save_best_only=True)\n",
    "\n",
    "subset_history = model.fit(\n",
    "    subset_train_data, subset_train_labels,\n",
    "    validation_data=(subset_val_data, subset_val_labels),\n",
    "    callbacks=[tensorboard_callback, subset_checkpoint],\n",
    "    batch_size=subset_batch_size,\n",
    "    epochs=subset_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training parameters\n",
    "main_batch_size = 32\n",
    "main_epochs = 100\n",
    "main_model_path = 'saved_model/main_model.keras'\n",
    "\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_mean_absolute_error',\n",
    "    patience=patience,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train on main dataset\n",
    "main_checkpoint = ModelCheckpoint(main_model_path, monitor='val_mean_absolute_error', save_best_only=True)\n",
    "\n",
    "main_history = model.fit(\n",
    "    train_data, train_labels,\n",
    "    validation_data=(valid_data, valid_labels),\n",
    "    callbacks=[tensorboard_callback, main_checkpoint,early_stopping],\n",
    "    batch_size=main_batch_size,\n",
    "    initial_learning_rate=main_initial_learning_rate,\n",
    "    epochs=main_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('saved_model/main_model.keras')\n",
    "\n",
    "# Overall evaluation\n",
    "loss, mae, mse, rmse, mape = loaded_model.evaluate(test_data, test_labels)\n",
    "print(f\"Overall Loss: {loss:.4f}\")\n",
    "print(f\"Overall MAE: {mae:.4f}\")\n",
    "print(f\"Overall MSE: {mse:.4f}\")\n",
    "print(f\"Overall RMSE: {rmse:.4f}\")\n",
    "print(f\"Overall MAPE: {mape:.4f}\")\n",
    "\n",
    "# Individual sample evaluation\n",
    "predictions = loaded_model.predict(test_data)\n",
    "errors = np.abs(predictions - test_labels).flatten()\n",
    "\n",
    "# Error statistics\n",
    "err68, err95 = np.percentile(errors, [68, 95])\n",
    "print(f\"68th percentile error (err68): {err68:.4f}\")\n",
    "print(f\"95th percentile error (err95): {err95:.4f}\")\n",
    "\n",
    "# Log metrics to TensorBoard\n",
    "with tf.summary.create_file_writer(log_dir).as_default():\n",
    "    tf.summary.scalar('Evaluation/Loss', loss, step=0)\n",
    "    tf.summary.scalar('Evaluation/MAE', mae, step=0)\n",
    "    tf.summary.scalar('Evaluation/MSE', mse, step=0)\n",
    "    tf.summary.scalar('Evaluation/RMSE', rmse, step=0)\n",
    "    tf.summary.scalar('Evaluation/MAPE', mape, step=0)\n",
    "    \n",
    "    for i, error in enumerate(errors):\n",
    "        tf.summary.scalar('Individual_MAE', error, step=i)\n",
    "    \n",
    "    tf.summary.scalar('Errors/err68', err68, step=0)\n",
    "    tf.summary.scalar('Errors/err95', err95, step=0)\n",
    "    tf.summary.histogram('Error_Distribution', errors, step=0)\n",
    "\n",
    "print(f\"TensorBoard logs saved to: {log_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Additional evaluation metrics\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
    "\n",
    "r2 = r2_score(test_labels, predictions)\n",
    "mape = mean_absolute_percentage_error(test_labels, predictions)\n",
    "\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "print(f\"MAPE: {mape:.4f}\")\n",
    "\n",
    "# Visualize predictions vs actual values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(test_labels, predictions, alpha=0.5)\n",
    "plt.plot([test_labels.min(), test_labels.max()], [test_labels.min(), test_labels.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predictions vs Actual Values')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(errors, bins=50)\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Error Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
