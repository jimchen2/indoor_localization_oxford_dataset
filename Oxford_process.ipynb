{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO5ynv-DKYxo",
        "outputId": "89a6ddbb-20f3-4c66-c5a1-1a6ba0bb25bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/magnetic\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd \"/content/drive/My Drive/magnetic\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import math\n",
        "from torch.nn.utils import weight_norm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from ipywidgets import interact, fixed\n",
        "from ipywidgets import widgets\n",
        "from ipywidgets import interactive, widgets\n",
        "from IPython.display import display\n",
        "from ipywidgets import interactive, widgets, HBox, VBox\n",
        "\n",
        "from datetime import datetime\n",
        "import socket\n"
      ],
      "metadata": {
        "id": "-kDuxL7dKhYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_directory = 'data/Oxford Inertial Odometry Dataset'\n",
        "kind = 'handheld'\n",
        "floor = 'data1'\n",
        "#seq_num = 1\n",
        "num_sequences = [7,3,5,5,4]\n",
        "num_folders = 1 #data1-4 for training, 5 for testing\n",
        "gt_type = 'vi'"
      ],
      "metadata": {
        "id": "ALXsvtQLKja0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE = 120\n",
        "STRIDE = 0\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "RBr5NyoYKnA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_gt_file(gt_csv_path):\n",
        "    path_data = {'x':[], 'y':[], \"z\":[]}\n",
        "\n",
        "    with open(gt_csv_path, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "\n",
        "        for row in reader:\n",
        "            x = - float(row[2])\n",
        "            y = float(row[3])\n",
        "            z = float(row[4])\n",
        "            path_data[\"x\"].append(x)\n",
        "            path_data[\"y\"].append(y)\n",
        "            path_data[\"z\"].append(z)\n",
        "\n",
        "\n",
        "    return path_data\n"
      ],
      "metadata": {
        "id": "go7Tb0f_KzBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gt_data = []\n",
        "for k in range(1, num_folders+1):\n",
        "  folder_path = os.path.join(root_directory, kind, f\"data{k}\")\n",
        "  print(folder_path)\n",
        "\n",
        "  gt_infolder = []\n",
        "  for i in range(1,num_sequences[k-1]+1):\n",
        "    gt_csv_path = os.path.join(folder_path, f\"syn/{gt_type}{i}.csv\")\n",
        "    print(gt_csv_path)\n",
        "    gt_infolder.append(process_gt_file(gt_csv_path))\n",
        "\n",
        "  gt_data.append(gt_infolder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1a4aaryK2E5",
        "outputId": "cbdd31ea-f84e-497c-b1ce-55961164af08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/Oxford Inertial Odometry Dataset/handheld/data1\n",
            "data/Oxford Inertial Odometry Dataset/handheld/data1/syn/vi1.csv\n",
            "data/Oxford Inertial Odometry Dataset/handheld/data1/syn/vi2.csv\n",
            "data/Oxford Inertial Odometry Dataset/handheld/data1/syn/vi3.csv\n",
            "data/Oxford Inertial Odometry Dataset/handheld/data1/syn/vi4.csv\n",
            "data/Oxford Inertial Odometry Dataset/handheld/data1/syn/vi5.csv\n",
            "data/Oxford Inertial Odometry Dataset/handheld/data1/syn/vi6.csv\n",
            "data/Oxford Inertial Odometry Dataset/handheld/data1/syn/vi7.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calMagFeature(mag, grav):\n",
        "    magnitude = math.sqrt(sum(component**2 for component in grav))\n",
        "    grav_norm = [component / magnitude for component in grav]\n",
        "    dot_product = sum(component1 * component2 for component1, component2 in zip(mag, grav_norm))\n",
        "    mag_along_grav = [component * dot_product for component in grav_norm]\n",
        "    mag_orth_grav = [component1 - component2 for component1, component2 in zip(mag, mag_along_grav)]\n",
        "    magnitide_along_grav = math.sqrt(sum(component**2 for component in mag_along_grav))\n",
        "    if dot_product<0:\n",
        "        magnitide_along_grav = -magnitide_along_grav\n",
        "    magnitide_orth_grav = math.sqrt(sum(component**2 for component in mag_orth_grav))\n",
        "    return [magnitide_along_grav,magnitide_orth_grav,math.sqrt(sum(component**2 for component in mag))]\n"
      ],
      "metadata": {
        "id": "uG6ijxdJLRYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_imu_file(imu_csv_path):\n",
        "    path_data = {'Bv':[], 'Bh':[], \"Bp\":[]}\n",
        "\n",
        "    with open(imu_csv_path, 'r') as file:\n",
        "        reader = csv.reader(file)\n",
        "\n",
        "        for row in reader:\n",
        "            x = float(row[-3])\n",
        "            y = float(row[-2])\n",
        "            z = float(row[-1])\n",
        "            PAA_mag = [x,y,z]\n",
        "            PAA_grav = [float(row[-9]),float(row[-8]),float(row[-7])]\n",
        "            Bv, Bh, Bp = calMagFeature(PAA_mag, PAA_grav)\n",
        "            path_data[\"Bv\"].append(Bv)\n",
        "            path_data[\"Bh\"].append(Bh)\n",
        "            path_data[\"Bp\"].append(Bp)\n",
        "\n",
        "\n",
        "    return path_data\n"
      ],
      "metadata": {
        "id": "2Q-yqQYdLTW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mag_data = []\n",
        "for k in range(1, num_folders+1):\n",
        "  folder_path = os.path.join(root_directory, kind, f\"data{k}\")\n",
        "  mag_infolder = []\n",
        "  for i in range(1,num_sequences[k-1]+1):\n",
        "    mag_csv_path = os.path.join(folder_path, f\"syn/imu{i}.csv\")\n",
        "    mag_infolder.append(process_imu_file(mag_csv_path))\n",
        "\n",
        "  mag_data.append(mag_infolder)"
      ],
      "metadata": {
        "id": "vkRhELvaLX_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(gt_data) == len(mag_data))\n",
        "for k in range(num_folders):\n",
        "\n",
        "  for i in range(num_sequences[k]):\n",
        "    if len(gt_data[k][i]) == len(mag_data[k][i]):\n",
        "      if (len(gt_data[k][i]['x']) == len(mag_data[k][i]['Bv'])):\n",
        "        #print(f'folder{k}, sequences{i+1} checked')\n",
        "        continue\n",
        "\n",
        "    print(f'Error!folder{k}, sequences{i+1}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoJ5YSVtLcZ3",
        "outputId": "5b8554b9-e6c1-4225-c146-368a6532f313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split**"
      ],
      "metadata": {
        "id": "9nhQ5ad5LfXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data1: contain the sequenced number used for training, validation and testing\n",
        "train_seq = [1,2,3,4,5]\n",
        "val_seq = [6]\n",
        "test_seq = [7]"
      ],
      "metadata": {
        "id": "KyTVj2KtLeng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_folder = [1,2,3,4]\n",
        "test_folder = [5]"
      ],
      "metadata": {
        "id": "By2RFJawLvu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sequences(mag_data, gt_data, window_size=WINDOW_SIZE, stride=STRIDE): #mag_data, gt_data: (sequence_idx, keys, values)\n",
        "\n",
        "  X, y = [], []\n",
        "  for idx, (mag, gt) in enumerate(zip(mag_data, gt_data)):\n",
        "    input = np.column_stack(((mag['Bv'], mag['Bh'], mag['Bp'])))\n",
        "    output = np.column_stack((gt['x'], gt['y']))\n",
        "\n",
        "    if input.shape[0]!= output.shape[0]:\n",
        "      print(f\"Error: input shape is {input.shape[0]}, output shape is {output.shape[0]}\")\n",
        "      return X, y\n",
        "\n",
        "    for i in range(len(input) - window_size):\n",
        "      if stride != 0 and i % stride != 0:\n",
        "          continue\n",
        "      else:\n",
        "        X.append(input[i:i+window_size])\n",
        "        y.append(output[i+window_size])\n",
        "        i += stride\n",
        "\n",
        "  return np.array(X), np.array(y)\n",
        "  #X : (num_samples, window_size, 3)\n",
        "  #y : (num_samples, 2)"
      ],
      "metadata": {
        "id": "nx-jBJGWLhHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mag_train_data  = []\n",
        "gt_train_data   = []\n",
        "mag_val_data  = []\n",
        "gt_val_data   = []\n",
        "mag_test_data  = []\n",
        "gt_test_data   = []"
      ],
      "metadata": {
        "id": "JMPpno6ILlLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 1\n",
        "for i in train_seq:\n",
        "  mag_train_data.append(mag_data[k-1][i-1])\n",
        "  gt_train_data.append(gt_data[k-1][i-1])\n",
        "\n",
        "for i in val_seq:\n",
        "  mag_val_data.append(mag_data[k-1][i-1])\n",
        "  gt_val_data.append(gt_data[k-1][i-1])\n",
        "\n",
        "for i in test_seq:\n",
        "  mag_test_data.append(mag_data[k-1][i-1])\n",
        "  gt_test_data.append(gt_data[k-1][i-1])"
      ],
      "metadata": {
        "id": "D2wDlfqHLog3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_raw, y_train_raw = prepare_sequences(mag_train_data, gt_train_data)\n",
        "X_val_raw, y_val_raw = prepare_sequences(mag_val_data, gt_val_data)\n",
        "X_test_raw, y_test_raw = prepare_sequences(mag_test_data, gt_test_data)"
      ],
      "metadata": {
        "id": "b_roctuNMKwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training raw data shape:\", X_train_raw.shape, y_train_raw.shape)\n",
        "print(\"Validation raw data shape:\", X_val_raw.shape, y_val_raw.shape)\n",
        "print(\"Testing raw data shape:\", X_test_raw.shape, y_test_raw.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07EXhHPCMlMY",
        "outputId": "df716b15-f411-46a2-a30c-18625258dd1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training raw data shape: (133099, 120, 3) (133099, 2)\n",
            "Validation raw data shape: (32417, 120, 3) (32417, 2)\n",
            "Testing raw data shape: (13978, 120, 3) (13978, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize input data\n",
        "scaler_X = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train_raw.reshape(-1, X_train_raw.shape[-1])).reshape(X_train_raw.shape)\n",
        "X_val_scaled = scaler_X.transform(X_val_raw.reshape(-1, X_val_raw.shape[-1])).reshape(X_val_raw.shape)\n",
        "X_test_scaled = scaler_X.transform(X_test_raw.reshape(-1, X_test_raw.shape[-1])).reshape(X_test_raw.shape)\n",
        "\n",
        "# Normalize output data\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train_raw)\n",
        "y_val_scaled = scaler_y.transform(y_val_raw)\n",
        "y_test_scaled = scaler_y.transform(y_test_raw)\n",
        "\n",
        "print(\"Training normalized data shape:\", X_train_scaled.shape, y_train_scaled.shape)\n",
        "print(\"Validation normalized data shape:\", X_val_scaled.shape, y_val_scaled.shape)\n",
        "print(\"Testing normalized data shape:\", X_test_scaled.shape, y_test_scaled.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8DmmSw9Mo0O",
        "outputId": "23346642-b4c0-48c6-9c3a-7c8ccf655370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training normalized data shape: (133099, 120, 3) (133099, 2)\n",
            "Validation normalized data shape: (32417, 120, 3) (32417, 2)\n",
            "Testing normalized data shape: (13978, 120, 3) (13978, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py"
      ],
      "metadata": {
        "id": "kBQ4UnOjMuyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder to store your data\n",
        "folder_name = \"data_hdf5\"\n",
        "folder_path = os.path.join(root_directory, folder_name)\n",
        "print(folder_path)\n",
        "if not os.path.exists(folder_path):\n",
        "  os.makedirs(folder_path, exist_ok=True)\n",
        "else:\n",
        "  print('Folder Exists')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLrRkMicMxRW",
        "outputId": "3b772c99-5908-4591-bd97-f900a396dd78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/Oxford Inertial Odometry Dataset/data_hdf5\n",
            "Folder Exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = os.path.join(root_directory, folder_name, f'dataset_reframe_data1_{WINDOW_SIZE}.hdf5')"
      ],
      "metadata": {
        "id": "hCgHujolNFTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and save the HDF5 file\n",
        "if not os.path.exists(dataset_name):\n",
        "  with h5py.File(dataset_name, 'w') as f:\n",
        "      # Save training data\n",
        "      f.create_dataset('train_data', data=X_train_scaled)\n",
        "      f.create_dataset('train_labels', data=y_train_scaled)\n",
        "\n",
        "      # Save validation data\n",
        "      f.create_dataset('val_data', data=X_val_scaled)\n",
        "      f.create_dataset('val_labels', data=y_val_scaled)\n",
        "\n",
        "      # Save testing data\n",
        "      f.create_dataset('test_data', data=X_test_scaled)\n",
        "      f.create_dataset('test_labels', data=y_test_scaled)\n",
        "else:\n",
        "  print(f'Dataset {dataset_name} Exists!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErKdcWE1M0Vr",
        "outputId": "ee5c596d-4782-474d-e8d7-8ab68e698708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset data/Oxford Inertial Odometry Dataset/data_hdf5/dataset_reframe_data1_120.hdf5 Exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File(dataset_name, 'r') as f:\n",
        "    # Load training data\n",
        "    train_data = torch.from_numpy(f['train_data'][:])\n",
        "    train_labels = torch.from_numpy(f['train_labels'][:])\n",
        "\n",
        "    # Load validation data\n",
        "    val_data = torch.from_numpy(f['val_data'][:])\n",
        "    val_labels = torch.from_numpy(f['val_labels'][:])\n",
        "\n",
        "    # Load testing data\n",
        "    test_data = torch.from_numpy(f['test_data'][:])\n",
        "    test_labels = torch.from_numpy(f['test_labels'][:])"
      ],
      "metadata": {
        "id": "s6fe3mglM8xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training normalized data shape:\", train_data.shape, train_labels.shape)\n",
        "print(\"Validation normalized data shape:\", val_data.shape, val_labels.shape)\n",
        "print(\"Testing normalized data shape:\", test_data.shape, test_labels.shape)"
      ],
      "metadata": {
        "id": "DKzPTpaRNh_6",
        "outputId": "48ba92a0-aa51-439d-c796-20173623e8b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training normalized data shape: torch.Size([133099, 120, 3]) torch.Size([133099, 2])\n",
            "Validation normalized data shape: torch.Size([32417, 120, 3]) torch.Size([32417, 2])\n",
            "Testing normalized data shape: torch.Size([13978, 120, 3]) torch.Size([13978, 2])\n"
          ]
        }
      ]
    }
  ]
}