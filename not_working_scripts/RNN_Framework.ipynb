{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using GPU: Tesla V100-SXM2-16GB\n",
      "1\n",
      "GPU 0: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "from torch.nn.utils import weight_norm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from ipywidgets import interact, fixed\n",
    "from ipywidgets import widgets\n",
    "from ipywidgets import interactive, widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interactive, widgets, HBox, VBox\n",
    "\n",
    "from datetime import datetime\n",
    "import socket\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Proprocess Data to Raw Data for Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIXEL_TO_METER_SCALE = 13.913\n",
    "image = Image.open(\"data/map/hkust_4f.jpg\")\n",
    "image = image.resize((int(image.size[0] / PIXEL_TO_METER_SCALE), \n",
    "                      int(image.size[1] / PIXEL_TO_METER_SCALE))).transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "def process_csv_file(csv_path, image_size, pixel_to_meter_scale):\n",
    "    path_data = {'x':[], 'y':[], \"Bv\":[], \"Bh\":[], \"Bp\":[]}\n",
    "    \n",
    "    with open(csv_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Skip the header\n",
    "        \n",
    "        for row in reader:\n",
    "            x = float(row[3]) / pixel_to_meter_scale\n",
    "            y = - float(row[4]) / pixel_to_meter_scale + image_size[1]\n",
    "            Bv = float(row[0])\n",
    "            Bh = float(row[1])\n",
    "            Bp = float(row[2])\n",
    "            \n",
    "            path_data[\"x\"].append(x)\n",
    "            path_data[\"y\"].append(y)\n",
    "            path_data[\"Bv\"].append(Bv)\n",
    "            path_data[\"Bh\"].append(Bh)\n",
    "            path_data[\"Bp\"].append(Bp)\n",
    "    \n",
    "    return path_data\n",
    "\n",
    "\n",
    "train_raw_data = []\n",
    "data_path = os.path.join(\".\", \"data\", \"formatted\", \"HKUST_4F\", \"training data\")\n",
    "\n",
    "for root, _, files in os.walk(data_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            train_raw_data.append(process_csv_file(os.path.join(root, file), image.size, PIXEL_TO_METER_SCALE))\n",
    "\n",
    "test_raw_data = []\n",
    "test_data_path = os.path.join(\".\", \"data\", \"formatted\", \"HKUST_4F\", \"testing data\")\n",
    "\n",
    "for root, _, files in os.walk(test_data_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            test_raw_data.append(process_csv_file(os.path.join(root, file), image.size, PIXEL_TO_METER_SCALE))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to print the first 10 entries of a trajectory\n",
    "def print_first_10_entries(traj, dataset_name):\n",
    "    print(f\"\\n{dataset_name} Trajectory:\")\n",
    "    for i in range(min(10, len(traj['x']))):\n",
    "        print(f\"Entry {i+1}: x={traj['x'][i]:.2f}, y={traj['y'][i]:.2f}, Bv={traj['Bv'][i]:.2f}, Bh={traj['Bh'][i]:.2f}, Bp={traj['Bp'][i]:.2f}\")\n",
    "\n",
    "# Randomly select 3 trajectories from the training dataset\n",
    "print(\"Training Dataset:\")\n",
    "for i in range(3):\n",
    "    traj = random.choice(train_raw_data)\n",
    "    print_first_10_entries(traj, f\"Training Trajectory {i+1}\")\n",
    "\n",
    "# Randomly select 3 trajectories from the testing dataset\n",
    "print(\"\\nTesting Dataset:\")\n",
    "for i in range(3):\n",
    "    traj = random.choice(test_raw_data)\n",
    "    print_first_10_entries(traj, f\"Testing Trajectory {i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Analyze Differences in Distributions between Train and Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_stats(data_list, key):\n",
    "    values = np.concatenate([np.array(d[key]) for d in data_list])\n",
    "    return {\n",
    "        'mean': np.mean(values),\n",
    "        'std': np.std(values),\n",
    "        'min': np.min(values),\n",
    "        'max': np.max(values)\n",
    "    }\n",
    "\n",
    "keys = ['x', 'y', 'Bv', 'Bh', 'Bp']\n",
    "\n",
    "print(\"Training Dataset Statistics:\")\n",
    "for key in keys:\n",
    "    stats = calculate_stats(train_raw_data, key)\n",
    "    print(f\"{key}: mean={stats['mean']:.2f}, std={stats['std']:.2f}, min={stats['min']:.2f}, max={stats['max']:.2f}\")\n",
    "\n",
    "print(\"Testing Dataset Statistics:\")\n",
    "for key in keys:\n",
    "    stats = calculate_stats(test_raw_data, key)\n",
    "    print(f\"{key}: mean={stats['mean']:.2f}, std={stats['std']:.2f}, min={stats['min']:.2f}, max={stats['max']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Length of the Raw Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_data_info(data, name):\n",
    "    total_length = sum(len(d['x']) for d in data)\n",
    "    individual_lengths = [len(d['x']) for d in data]\n",
    "    num_trajectories = len(data)\n",
    "    \n",
    "    print(f\"{name} data:\")\n",
    "    print(f\"  Total length: {total_length}\")\n",
    "    print(f\"  Individual lengths: {individual_lengths}\")\n",
    "    print(f\"  Number of trajectories: {num_trajectories}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print_data_info(train_raw_data, \"Train Raw\")\n",
    "print_data_info(test_raw_data, \"Test Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_data(data):\n",
    "    def update_plot(index):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        \n",
    "        # Scatter plot for all points\n",
    "        plt.scatter(data[index][\"x\"], data[index][\"y\"], s=30, alpha=0.5, label='Steps')\n",
    "        \n",
    "        # Highlight start and end points\n",
    "        plt.scatter(data[index][\"x\"][0], data[index][\"y\"][0], color='green', s=100, label='Start')\n",
    "        plt.scatter(data[index][\"x\"][-1], data[index][\"y\"][-1], color='red', s=100, label='End')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.title(f\"Trajectory at index {index}\")\n",
    "        plt.xlabel(\"X coordinate\")\n",
    "        plt.ylabel(\"Y coordinate\")\n",
    "        plt.grid(True)\n",
    "        plt.axis('equal')  # This ensures the aspect ratio is 1:1\n",
    "        plt.show()\n",
    "\n",
    "    slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=len(data) - 1,\n",
    "        step=1,\n",
    "        description='Index:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "\n",
    "    widget = widgets.interactive(update_plot, index=slider)\n",
    "    display(widget)\n",
    "\n",
    "visualize_data(train_raw_data)\n",
    "# visualize_data(test_raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Process Data to Sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (131043, 30, 3) (131043, 2)\n",
      "Validation data shape: (21965, 30, 3) (21965, 2)\n",
      "Testing data shape: (17777, 30, 3) (17777, 2)\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequences(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for traj in data:\n",
    "        input_seq = np.column_stack((traj['Bv'], traj['Bh'], traj['Bp']))\n",
    "        output_seq = np.column_stack((traj['x'], traj['y']))\n",
    "        \n",
    "        for i in range(len(input_seq) - sequence_length):\n",
    "            X.append(input_seq[i:i+sequence_length])\n",
    "            y.append(output_seq[i+sequence_length])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Set sequence length\n",
    "sequence_length = 30\n",
    "\n",
    "# Split train_raw_data into train and validation sets at the trajectory level\n",
    "train_trajectories, val_trajectories = train_test_split(train_raw_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare sequences for each set\n",
    "X_train_seq, y_train_seq = prepare_sequences(train_trajectories, sequence_length)\n",
    "X_val_seq, y_val_seq = prepare_sequences(val_trajectories, sequence_length)\n",
    "X_test_seq, y_test_seq = prepare_sequences(test_raw_data, sequence_length)\n",
    "\n",
    "# Normalize input data\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train_seq.reshape(-1, X_train_seq.shape[-1])).reshape(X_train_seq.shape)\n",
    "X_val_scaled = scaler_X.transform(X_val_seq.reshape(-1, X_val_seq.shape[-1])).reshape(X_val_seq.shape)\n",
    "X_test_scaled = scaler_X.transform(X_test_seq.reshape(-1, X_test_seq.shape[-1])).reshape(X_test_seq.shape)\n",
    "\n",
    "# Normalize output data\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_seq)\n",
    "y_val_scaled = scaler_y.transform(y_val_seq)\n",
    "y_test_scaled = scaler_y.transform(y_test_seq)\n",
    "\n",
    "print(\"Training data shape:\", X_train_scaled.shape, y_train_scaled.shape)\n",
    "print(\"Validation data shape:\", X_val_scaled.shape, y_val_scaled.shape)\n",
    "print(\"Testing data shape:\", X_test_scaled.shape, y_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *What you get at this step is sequence of data of `Bv`, `Bp` and `Bh` followed by an `x` and `y` output*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plot and Visualize Sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_single_dataset(index, data_type, dataset):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    if data_type == 'Raw':\n",
    "        X = X_train if dataset == 'Train' else X_val if dataset == 'Validation' else X_test\n",
    "        y = y_train if dataset == 'Train' else y_val if dataset == 'Validation' else y_test\n",
    "    else:\n",
    "        X = X_train_scaled if dataset == 'Train' else X_val_scaled if dataset == 'Validation' else X_test_scaled\n",
    "        y = y_train_scaled if dataset == 'Train' else y_val_scaled if dataset == 'Validation' else y_test_scaled\n",
    "\n",
    "    # Plot input sequence\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f'{data_type} Input Sequence - {dataset}')\n",
    "    plt.plot(X[index, :, 0], label='Bv')\n",
    "    plt.plot(X[index, :, 1], label='Bh')\n",
    "    plt.plot(X[index, :, 2], label='Bp')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Value')\n",
    "\n",
    "    # Plot output\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f'{data_type} Output - {dataset}')\n",
    "    plt.scatter(y[index, 0], y[index, 1], color='red', label='Position')\n",
    "    plt.legend()\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_interactive_plot(dataset):\n",
    "    max_index = len(X_train)-1 if dataset == 'Train' else len(X_val)-1 if dataset == 'Validation' else len(X_test)-1\n",
    "    \n",
    "    index_input = widgets.BoundedIntText(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=max_index,\n",
    "        description=f'{dataset} Index:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    index_slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=max_index,\n",
    "        description='Progress:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Link the input and slider\n",
    "    widgets.jslink((index_input, 'value'), (index_slider, 'value'))\n",
    "    \n",
    "    data_type_widget = widgets.RadioButtons(options=['Raw', 'Scaled'], description='Data Type:')\n",
    "    \n",
    "    def update_plot(index, data_type):\n",
    "        plot_single_dataset(index, data_type, dataset)\n",
    "    \n",
    "    interactive_plot = interactive(update_plot, index=index_input, data_type=data_type_widget)\n",
    "    \n",
    "    return VBox([\n",
    "        widgets.HTML(f\"<h3>{dataset} Dataset</h3>\"),\n",
    "        HBox([index_input, index_slider]),\n",
    "        data_type_widget,\n",
    "        interactive_plot.children[-1]\n",
    "    ])\n",
    "\n",
    "# Create and display interactive plots for each dataset\n",
    "train_plot = create_interactive_plot('Train')\n",
    "val_plot = create_interactive_plot('Validation')\n",
    "test_plot = create_interactive_plot('Test')\n",
    "\n",
    "display(train_plot, val_plot, test_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define the dataset and dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TrajectoryDataset(X_train_scaled, y_train_scaled)\n",
    "val_dataset = TrajectoryDataset(X_val_scaled, y_val_scaled)\n",
    "test_dataset = TrajectoryDataset(X_test_scaled, y_test_scaled)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 4096\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # It's already shuffled but doesnt hurt though\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define Training Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_original_losses(loader, model, scaler, dataset_name):\n",
    "    total_mse = 0\n",
    "    total_mae = 0\n",
    "    total_samples = 0\n",
    "    mse_criterion = nn.MSELoss(reduction='mean')\n",
    "    mae_criterion = nn.L1Loss(reduction='mean')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            \n",
    "            outputs_original = torch.from_numpy(scaler_y.inverse_transform(outputs.cpu().numpy())).to(device)\n",
    "            batch_y_original = torch.from_numpy(scaler_y.inverse_transform(batch_y.cpu().numpy())).to(device)\n",
    "            \n",
    "            mse = mse_criterion(outputs_original, batch_y_original)\n",
    "            mae = mae_criterion(outputs_original, batch_y_original)\n",
    "            \n",
    "            total_mse += mse.item() * batch_y.size(0)\n",
    "            total_mae += mae.item() * batch_y.size(0)\n",
    "            total_samples += batch_y.size(0)\n",
    "    \n",
    "    mse = total_mse / total_samples\n",
    "    mae = total_mae / total_samples\n",
    "    print(f\"Original {dataset_name} MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "    return mse, mae\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, device, num_epochs, log_original_loss_every, scaler_y, patience):\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in pbar:\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = nn.MSELoss()(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                loss =  nn.MSELoss()(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'Epoch': epoch+1,\n",
    "            'Train Loss': f'{train_loss:.4f}',\n",
    "            'Val Loss': f'{val_loss:.4f}'\n",
    "        })\n",
    "        \n",
    "        if (epoch + 1) % log_original_loss_every == 0:\n",
    "            train_mse, train_mae = calculate_original_losses(train_loader, model, scaler_y, \"Train\")\n",
    "            val_mse, val_mae = calculate_original_losses(val_loader, model, scaler_y, \"Val\")\n",
    "            # test_mse, test_mae = calculate_original_losses(test_loader, model, scaler_y, \"Test\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve == patience:\n",
    "                print(\"\\nEarly stopping!\")\n",
    "                break\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Some Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_epochs = 200\n",
    "best_val_loss = float('inf')\n",
    "patience = 100\n",
    "no_improve = 0\n",
    "log_original_loss_every = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Use GRU RNN model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:03<?, ?it/s, Epoch=1, Train Loss=0.9468, Val Loss=0.8687]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 965.4396, MAE: 23.2291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:06<20:41,  6.24s/it, Epoch=1, Train Loss=0.9468, Val Loss=0.8687]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 1034.0105, MAE: 23.9080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:10<20:41,  6.24s/it, Epoch=2, Train Loss=0.9152, Val Loss=0.8073]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 940.1622, MAE: 22.7325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:12<20:16,  6.14s/it, Epoch=2, Train Loss=0.9152, Val Loss=0.8073]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 945.1053, MAE: 23.0318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:16<20:16,  6.14s/it, Epoch=3, Train Loss=0.8927, Val Loss=0.8142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 925.8621, MAE: 22.6385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/200 [00:18<20:21,  6.20s/it, Epoch=3, Train Loss=0.8927, Val Loss=0.8142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 955.4967, MAE: 23.2172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/200 [00:22<20:21,  6.20s/it, Epoch=4, Train Loss=0.8854, Val Loss=0.8297]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 921.9031, MAE: 22.5731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/200 [00:24<20:02,  6.13s/it, Epoch=4, Train Loss=0.8854, Val Loss=0.8297]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 971.6613, MAE: 23.3632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/200 [00:28<20:02,  6.13s/it, Epoch=5, Train Loss=0.8729, Val Loss=0.8079]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 906.8912, MAE: 22.2065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 5/200 [00:30<20:00,  6.15s/it, Epoch=5, Train Loss=0.8729, Val Loss=0.8079]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 931.3238, MAE: 22.9686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 5/200 [00:34<20:00,  6.15s/it, Epoch=6, Train Loss=0.8447, Val Loss=0.8201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 881.5166, MAE: 21.6866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6/200 [00:36<19:54,  6.16s/it, Epoch=6, Train Loss=0.8447, Val Loss=0.8201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 938.5343, MAE: 22.9818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6/200 [00:40<19:54,  6.16s/it, Epoch=7, Train Loss=0.8176, Val Loss=0.8135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 858.6225, MAE: 21.1617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 7/200 [00:43<19:40,  6.12s/it, Epoch=7, Train Loss=0.8176, Val Loss=0.8135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 948.9588, MAE: 22.7870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 7/200 [00:46<19:40,  6.12s/it, Epoch=8, Train Loss=0.7936, Val Loss=0.8362]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 839.5077, MAE: 20.9300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [00:49<19:28,  6.09s/it, Epoch=8, Train Loss=0.7936, Val Loss=0.8362]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 991.5414, MAE: 23.3249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [00:52<19:28,  6.09s/it, Epoch=9, Train Loss=0.7703, Val Loss=0.8201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 820.4206, MAE: 20.3928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9/200 [00:55<19:29,  6.12s/it, Epoch=9, Train Loss=0.7703, Val Loss=0.8201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 949.9343, MAE: 22.5371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9/200 [00:59<19:29,  6.12s/it, Epoch=10, Train Loss=0.7445, Val Loss=0.8236]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 794.1270, MAE: 20.0087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [01:01<19:17,  6.09s/it, Epoch=10, Train Loss=0.7445, Val Loss=0.8236]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 952.5234, MAE: 22.6084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [01:05<19:17,  6.09s/it, Epoch=11, Train Loss=0.7222, Val Loss=0.8653]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 769.5388, MAE: 19.6974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [01:07<19:17,  6.13s/it, Epoch=11, Train Loss=0.7222, Val Loss=0.8653]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 1004.7440, MAE: 23.1383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [01:11<19:17,  6.13s/it, Epoch=12, Train Loss=0.6975, Val Loss=0.8921]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 744.6764, MAE: 19.1807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 12/200 [01:13<19:07,  6.10s/it, Epoch=12, Train Loss=0.6975, Val Loss=0.8921]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 1043.1075, MAE: 23.4499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 12/200 [01:17<19:07,  6.10s/it, Epoch=13, Train Loss=0.6703, Val Loss=0.8641]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 718.1625, MAE: 18.6190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 13/200 [01:19<18:59,  6.09s/it, Epoch=13, Train Loss=0.6703, Val Loss=0.8641]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 1007.8255, MAE: 22.6597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 13/200 [01:23<18:59,  6.09s/it, Epoch=14, Train Loss=0.6455, Val Loss=0.8976]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 679.3737, MAE: 18.1517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 14/200 [01:25<19:02,  6.14s/it, Epoch=14, Train Loss=0.6455, Val Loss=0.8976]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 1050.2199, MAE: 23.3007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 14/200 [01:29<19:02,  6.14s/it, Epoch=15, Train Loss=0.6193, Val Loss=0.8869]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 660.0238, MAE: 17.7035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 15/200 [01:32<19:01,  6.17s/it, Epoch=15, Train Loss=0.6193, Val Loss=0.8869]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 1010.3936, MAE: 22.7843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 15/200 [01:37<20:03,  6.51s/it, Epoch=16, Train Loss=0.5924, Val Loss=0.9512]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_num_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_original_loss_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_original_loss_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 67\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, device, num_epochs, log_original_loss_every, scaler_y, patience)\u001b[0m\n\u001b[1;32m     60\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix({\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVal Loss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     64\u001b[0m })\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m log_original_loss_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 67\u001b[0m     train_mse, train_mae \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_original_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     val_mse, val_mae \u001b[38;5;241m=\u001b[39m calculate_original_losses(val_loader, model, scaler_y, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# test_mse, test_mae = calculate_original_losses(test_loader, model, scaler_y, \"Test\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mcalculate_original_losses\u001b[0;34m(loader, model, scaler, dataset_name)\u001b[0m\n\u001b[1;32m      6\u001b[0m mae_criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mL1Loss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class TrajectoryRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=3):  # Changed default to 3 layers\n",
    "        super(TrajectoryRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        output = self.fc(hidden[-1])  # Use the last layer's hidden state\n",
    "        return output\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 3  # Bv, Bh, Bp\n",
    "hidden_size = 256\n",
    "output_size = 2  # x, y\n",
    "num_layers = 3  # Specify the number of layers you want\n",
    "model = TrajectoryRNN(input_size, hidden_size, output_size, num_layers=num_layers).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Call the function\n",
    "model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=max_num_epochs,\n",
    "    log_original_loss_every=log_original_loss_every,\n",
    "    scaler_y=scaler_y,\n",
    "    patience=patience\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Variant of GRU Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:29<32:12,  9.76s/it, Epoch=3, Train Loss=0.8587, Val Loss=0.8544]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 901.3625, MAE: 22.0700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/200 [00:33<38:36, 11.76s/it, Epoch=3, Train Loss=0.8587, Val Loss=0.8544]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 906.3299, MAE: 22.0983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 5/200 [01:02<34:00, 10.47s/it, Epoch=6, Train Loss=0.8254, Val Loss=0.8298]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 880.3318, MAE: 21.5404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6/200 [01:07<38:06, 11.79s/it, Epoch=6, Train Loss=0.8254, Val Loss=0.8298]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 886.7945, MAE: 21.5868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/200 [01:36<34:07, 10.66s/it, Epoch=9, Train Loss=0.7922, Val Loss=0.7917]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train MSE: 854.0382, MAE: 20.9462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9/200 [01:41<37:42, 11.85s/it, Epoch=9, Train Loss=0.7922, Val Loss=0.7917]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Val MSE: 866.1204, MAE: 21.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9/200 [01:49<38:46, 12.18s/it, Epoch=9, Train Loss=0.7922, Val Loss=0.7917]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_num_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_original_loss_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_original_loss_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, device, num_epochs, log_original_loss_every, scaler_y, patience)\u001b[0m\n\u001b[1;32m     42\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     43\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 45\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     49\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class TrajectoryRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=3):\n",
    "        super(TrajectoryRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # Add batch normalization after RNN\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Add a fully connected layer with ReLU activation\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Add another batch normalization layer\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size // 2)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        output = hidden[-1]  # Use the last layer's hidden state\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        output = self.bn(output)\n",
    "        \n",
    "        # Apply first fully connected layer and ReLU\n",
    "        output = self.fc1(output)\n",
    "        output = self.relu(output)\n",
    "        \n",
    "        # Apply second batch normalization\n",
    "        output = self.bn2(output)\n",
    "        \n",
    "        # Apply final output layer\n",
    "        output = self.fc2(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 3  # Bv, Bh, Bp\n",
    "hidden_size = 512\n",
    "output_size = 2  # x, y\n",
    "num_layers = 3  # Specify the number of layers you want\n",
    "model = TrajectoryRNN(input_size, hidden_size, output_size, num_layers=num_layers).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Call the function\n",
    "model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=max_num_epochs,\n",
    "    log_original_loss_every=log_original_loss_every,\n",
    "    scaler_y=scaler_y,\n",
    "    patience=patience\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Use LSTM based Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=3):\n",
    "        super(TrajectoryLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        output = self.fc(hidden[-1])  # Use the last layer's hidden state\n",
    "        return output\n",
    "\n",
    "# Initialize the LSTM model\n",
    "input_size = 3  # Bv, Bh, Bp\n",
    "hidden_size = 512\n",
    "output_size = 2  # x, y\n",
    "num_layers = 4\n",
    "lstm_model = TrajectoryLSTM(input_size, hidden_size, output_size, num_layers=num_layers).to(device)\n",
    "\n",
    "# Define loss function and optimizer for LSTM\n",
    "lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the LSTM model\n",
    "lstm_model = train_model(\n",
    "    model=lstm_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=lstm_optimizer,\n",
    "    device=device,\n",
    "    num_epochs=max_num_epochs,\n",
    "    log_original_loss_every=log_original_loss_every,\n",
    "    scaler_y=scaler_y,\n",
    "    patience=patience\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Use TCN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.bn1 = nn.BatchNorm1d(n_outputs)\n",
    "        self.leaky_relu1 = nn.LeakyReLU(0.01)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.bn2 = nn.BatchNorm1d(n_outputs)\n",
    "        self.leaky_relu2 = nn.LeakyReLU(0.01)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.bn1, self.leaky_relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.bn2, self.leaky_relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.leaky_relu(out + res)\n",
    "\n",
    "class TCNTrajectoryModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCNTrajectoryModel, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x needs to have dimension (N, C, L) in order to be passed into CNN\n",
    "        y1 = self.tcn(x.transpose(1, 2)).transpose(1, 2)\n",
    "        y2 = self.flatten(y1[:, -1, :].unsqueeze(1))\n",
    "        return self.linear(y2)\n",
    "\n",
    "# Initialize the TCN model\n",
    "input_size = 3  # Bv, Bh, Bp\n",
    "output_size = 2  # x, y\n",
    "num_channels = [64, 128, 256, 128, 64]  # Increased number of channels and layers\n",
    "kernel_size = 3\n",
    "dropout = 0.2\n",
    "\n",
    "tcn_model = TCNTrajectoryModel(input_size, output_size, num_channels, kernel_size, dropout).to(device)\n",
    "\n",
    "tcn_optimizer = torch.optim.Adam(tcn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the TCN model\n",
    "tcn_model = train_model(\n",
    "    model=tcn_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=tcn_optimizer,\n",
    "    device=device,\n",
    "    num_epochs=max_num_epochs,\n",
    "    log_original_loss_every=log_original_loss_every,\n",
    "    scaler_y=scaler_y,\n",
    "    patience=patience\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transformer Based Model (todo)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluate the model on Test Set (todo: buggy and problematic here)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_mse, test_mae = calculate_original_losses(test_loader, model, scaler_y, \"Test\")\n",
    "\n",
    "print(f\"Final Test MSE: {test_mse:.4f}\")\n",
    "print(f\"Final Test MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Interactive Visulization (todo)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save the Model (todo)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# unique_id = f\"{socket.gethostname()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "# dest_path = f\"saved_models/best_model_{unique_id}.pth\"\n",
    "\n",
    "# os.makedirs('saved_models', exist_ok=True)\n",
    "# !cp best_model.pth {dest_path}\n",
    "\n",
    "# print(f\"Best model copied and saved as '{dest_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
